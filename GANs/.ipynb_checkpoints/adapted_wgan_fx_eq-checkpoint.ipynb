{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df8142",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/mnt/data/raw (FX + EQ).csv\")\n",
    "\n",
    "# Calculate log returns\n",
    "log_returns = np.log(df / df.shift(1)).dropna()\n",
    "\n",
    "# Estimate mean and covariance of the real log returns\n",
    "mean_returns = log_returns.mean().values\n",
    "cov_returns = log_returns.cov().values\n",
    "\n",
    "# Generate synthetic returns using multivariate normal\n",
    "synthetic_returns_mvnorm = multivariate_normal(mean_returns, cov_returns, size=log_returns.shape[0])\n",
    "\n",
    "# Convert to synthetic price paths\n",
    "synthetic_prices_mvnorm = pd.DataFrame(np.exp(np.cumsum(synthetic_returns_mvnorm, axis=0)),\n",
    "                                       columns=log_returns.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6603ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot real vs synthetic price paths\n",
    "fig, axes = plt.subplots(3, 4, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(log_returns.columns):\n",
    "    real_prices = np.exp(np.cumsum(log_returns[col].values))\n",
    "    synthetic = synthetic_prices_mvnorm[col].values\n",
    "\n",
    "    axes[i].plot(real_prices, label='Real', alpha=0.7)\n",
    "    axes[i].plot(synthetic, label='Synthetic', alpha=0.7)\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d78ff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/mnt/data/raw (FX + EQ).csv\")\n",
    "\n",
    "# Calculate log returns\n",
    "log_returns = np.log(df / df.shift(1)).dropna()\n",
    "\n",
    "# Estimate mean and covariance of the real log returns\n",
    "mean_returns = log_returns.mean().values\n",
    "cov_returns = log_returns.cov().values\n",
    "\n",
    "# Generate synthetic returns using multivariate normal\n",
    "synthetic_returns_mvnorm = multivariate_normal(mean_returns, cov_returns, size=log_returns.shape[0])\n",
    "\n",
    "# Convert to synthetic price paths\n",
    "synthetic_prices_mvnorm = pd.DataFrame(np.exp(np.cumsum(synthetic_returns_mvnorm, axis=0)),\n",
    "                                       columns=log_returns.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a17383",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot real vs synthetic price paths\n",
    "fig, axes = plt.subplots(3, 4, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(log_returns.columns):\n",
    "    real_prices = np.exp(np.cumsum(log_returns[col].values))\n",
    "    synthetic = synthetic_prices_mvnorm[col].values\n",
    "\n",
    "    axes[i].plot(real_prices, label='Real', alpha=0.7)\n",
    "    axes[i].plot(synthetic, label='Synthetic', alpha=0.7)\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed11b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a toy dataset of points on a circle with some noise\n",
    "class CircleDataset(Dataset):\n",
    "    def __init__(self, n_samples=1000):\n",
    "        # Generate angles\n",
    "        theta = np.linspace(0, 2*np.pi, n_samples)\n",
    "        # Add some noise to radius\n",
    "        r = np.random.normal(1, 0.1, n_samples)\n",
    "        # Convert to cartesian coordinates\n",
    "        self.data = torch.FloatTensor([\n",
    "            [r[i] * np.cos(theta[i]), r[i] * np.sin(theta[i])]\n",
    "            for i in range(n_samples)\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Generator Network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=2):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)  # Output 2D points\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# Critic Network (Discriminator in WGAN)\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)  # Output a single value (no sigmoid)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Training function\n",
    "def train_wgan(generator, critic, dataloader, num_epochs=10000, n_critic=5, \n",
    "               clip_value=0.01, lr=0.00005, latent_dim=2, device=\"cpu\"):\n",
    "    \n",
    "    # Optimizers\n",
    "    optimizer_G = optim.RMSprop(generator.parameters(), lr=lr)\n",
    "    optimizer_C = optim.RMSprop(critic.parameters(), lr=lr)\n",
    "    \n",
    "    # Lists to store losses for plotting\n",
    "    G_losses = []\n",
    "    C_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, real_data in enumerate(dataloader):\n",
    "            batch_size = real_data.size(0)\n",
    "            real_data = real_data.to(device)\n",
    "\n",
    "            # Train Critic\n",
    "            for _ in range(n_critic):\n",
    "                optimizer_C.zero_grad()\n",
    "                \n",
    "                # Generate fake data\n",
    "                z = torch.randn(batch_size, latent_dim).to(device)\n",
    "                fake_data = generator(z)\n",
    "                \n",
    "                # Compute critic loss\n",
    "                critic_real = critic(real_data).mean()\n",
    "                critic_fake = critic(fake_data.detach()).mean()\n",
    "                critic_loss = -(critic_real - critic_fake)\n",
    "                \n",
    "                critic_loss.backward()\n",
    "                optimizer_C.step()\n",
    "                \n",
    "                # Clip critic weights\n",
    "                for p in critic.parameters():\n",
    "                    p.data.clamp_(-clip_value, clip_value)\n",
    "            \n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            # Generate fake data\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_data = generator(z)\n",
    "            \n",
    "            # Compute generator loss\n",
    "            generator_loss = -critic(fake_data).mean()\n",
    "            \n",
    "            generator_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            if i == 0:  # Save losses once per epoch\n",
    "                G_losses.append(generator_loss.item())\n",
    "                C_losses.append(critic_loss.item())\n",
    "                \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "                  f\"Critic Loss: {critic_loss.item():.4f} \"\n",
    "                  f\"Generator Loss: {generator_loss.item():.4f}\")\n",
    "    \n",
    "    return G_losses, C_losses\n",
    "\n",
    "# Function to visualize results\n",
    "def plot_results(generator, dataset, n_samples=1000, latent_dim=2):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        # Generate fake samples\n",
    "        z = torch.randn(n_samples, latent_dim)\n",
    "        fake_samples = generator(z).numpy()\n",
    "        \n",
    "        # Get real samples\n",
    "        real_samples = dataset.data.numpy()\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(real_samples[:, 0], real_samples[:, 1], c='blue', alpha=0.5, label='Real')\n",
    "        plt.title('Real Data Distribution')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(fake_samples[:, 0], fake_samples[:, 1], c='red', alpha=0.5, label='Fake')\n",
    "        plt.title('Generated Data Distribution')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Main training routine\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    batch_size = 64\n",
    "    latent_dim = 2\n",
    "    n_critic = 5\n",
    "    clip_value = 0.01\n",
    "    lr = 0.00005\n",
    "    num_epochs = 2000\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = CircleDataset()\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize networks\n",
    "    generator = Generator(latent_dim)\n",
    "    critic = Critic()\n",
    "    \n",
    "    # Train the model\n",
    "    G_losses, C_losses = train_wgan(\n",
    "        generator, critic, dataloader,\n",
    "        num_epochs=num_epochs,\n",
    "        n_critic=n_critic,\n",
    "        clip_value=clip_value,\n",
    "        lr=lr,\n",
    "        latent_dim=latent_dim\n",
    "    )\n",
    "    \n",
    "    # Plot training losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(G_losses, label='Generator Loss')\n",
    "    plt.plot(C_losses, label='Critic Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot real vs generated distributions\n",
    "    plot_results(generator, dataset)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f2d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
