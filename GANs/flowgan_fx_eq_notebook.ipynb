{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54a22583-3545-40b0-8652-fed48bc7e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TensorFlow 1.x compatibility\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import regularizers \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load FX/EQ dataset\n",
    "df = pd.read_csv(\"raw (FX + EQ).csv\").drop(columns=[\"Date\"], errors=\"ignore\").dropna()\n",
    "data = df.values\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Create rolling windows\n",
    "window_size = 30\n",
    "num_series = data.shape[1]\n",
    "X = []\n",
    "for i in range(len(scaled_data) - window_size):\n",
    "    window = scaled_data[i:i+window_size]\n",
    "    X.append(window)\n",
    "X = np.array(X).reshape(-1, window_size, num_series, 1)\n",
    "\n",
    "# Save to disk for FlowGAN\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "np.save(\"data/fxeq_train.npy\", X)\n",
    "\n",
    "def construct_model_spec(scale_init=2, no_of_layers=8, add_scaling=True):\n",
    "  global layers\n",
    "  num_scales = scale_init\n",
    "  for scale in range(num_scales-1):    \n",
    "    layers.append(CouplingLayer('checkerboard0', name='Checkerboard%d_1' % scale, \n",
    "      num_residual_blocks=no_of_layers, scaling=add_scaling))\n",
    "    layers.append(CouplingLayer('checkerboard1', name='Checkerboard%d_2' % scale, \n",
    "      num_residual_blocks=no_of_layers, scaling=add_scaling))\n",
    "    layers.append(CouplingLayer('checkerboard0', name='Checkerboard%d_3' % scale, \n",
    "      num_residual_blocks=no_of_layers, scaling=add_scaling))\n",
    "    layers.append(SqueezingLayer(name='Squeeze%d' % scale))\n",
    "    layers.append(CouplingLayer('channel0', name='Channel%d_1' % scale, \n",
    "      num_residual_blocks=no_of_layers, scaling=add_scaling))\n",
    "    layers.append(CouplingLayer('channel1', name='Channel%d_2' % scale, \n",
    "      num_residual_blocks=no_of_layers, scaling=add_scaling))\n",
    "    layers.append(CouplingLayer('channel0', name='Channel%d_3' % scale, \n",
    "      num_residual_blocks=no_of_layers, scaling=add_scaling))\n",
    "    layers.append(FactorOutLayer(scale, name='FactorOut%d' % scale))\n",
    "\n",
    "  # # final layer\n",
    "  scale = num_scales-1\n",
    "  layers.append(CouplingLayer('checkerboard0', name='Checkerboard%d_1' % scale,\n",
    "      num_residual_blocks=no_of_layers, scaling=add_scaling))\n",
    "  layers.append(CouplingLayer('checkerboard1', name='Checkerboard%d_2' % scale,\n",
    "      num_residual_blocks=no_of_layers, scaling=add_scaling))\n",
    "  layers.append(CouplingLayer('checkerboard0', name='Checkerboard%d_3' % scale,\n",
    "      num_residual_blocks=no_of_layers, scaling=add_scaling))\n",
    "  layers.append(CouplingLayer('checkerboard1', name='Checkerboard%d_4' % scale,\n",
    "      num_residual_blocks=no_of_layers, scaling=add_scaling))\n",
    "  layers.append(FactorOutLayer(scale, name='FactorOut%d' % scale))\n",
    "\n",
    "def construct_nice_spec(init_type=\"uniform\", hidden_layers=1000, no_of_layers=1):\n",
    "  global layers\n",
    "\n",
    "  layers.append(NICECouplingLayer('checkerboard0', name='Checkerboard_1', seed=0, \n",
    "    init_type=init_type, hidden_layers=hidden_layers, no_of_layers=no_of_layers))\n",
    "  layers.append(NICECouplingLayer('checkerboard1', name='Checkerboard_2', seed=1, \n",
    "    init_type=init_type, hidden_layers=hidden_layers, no_of_layers=no_of_layers))\n",
    "  layers.append(NICECouplingLayer('checkerboard0', name='Checkerboard_3', seed=2, \n",
    "    init_type=init_type, hidden_layers=hidden_layers, no_of_layers=no_of_layers))\n",
    "  layers.append(NICECouplingLayer('checkerboard1', name='Checkerboard_4', seed=3, \n",
    "    init_type=init_type, hidden_layers=hidden_layers, no_of_layers=no_of_layers))\n",
    "  layers.append(NICEScaling(name='Scaling', seed=4))\n",
    "\n",
    "\n",
    "class Config:\n",
    "    input_file = \"data/fxeq_train.npy\"\n",
    "    batch_size = 64\n",
    "    input_height = 30\n",
    "    input_width = X.shape[2]\n",
    "    c_dim = 1\n",
    "    epoch = 20\n",
    "    learning_rate = 0.0001\n",
    "    beta1 = 0.5\n",
    "    dataset = \"fxeq\"\n",
    "    checkpoint_dir = \"checkpoint_fxeq\"\n",
    "    sample_dir = \"samples_fxeq\"\n",
    "    flow_log_dir = \"logs\"\n",
    "    f_div = \"wgan\"\n",
    "    prior = \"gaussian\"\n",
    "    alpha = 1e-7\n",
    "    lr_decay = 1.0\n",
    "    min_lr = 0.0\n",
    "    reg = 10.0\n",
    "    model_type = \"real_nvp\"\n",
    "    init_type = \"normal\"\n",
    "    n_critic = 5\n",
    "    batch_norm_adaptive = 1\n",
    "    no_of_layers = 8\n",
    "    hidden_layers = 1000\n",
    "    df_dim = 64\n",
    "    like_reg = 1.0\n",
    "\n",
    "FLAGS = Config()\n",
    "\n",
    "layers = []\n",
    "final_latent_dimension = []\n",
    "def model_spec(x, reuse=True, model_type=\"nice\", train=False, \n",
    "               alpha=1e-7, init_type=\"uniform\", hidden_layers=1000, \n",
    "               no_of_layers=1, batch_norm_adaptive=0):\n",
    "    global layers, final_latent_dimension\n",
    "\n",
    "    if not reuse:\n",
    "        layers = []  # <-- THIS FIXES THE PROBLEM\n",
    "\n",
    "    counters = {}\n",
    "    xs = int_shape(x)\n",
    "    sum_log_det_jacobians = tf.zeros(xs[0])\n",
    "\n",
    "    # corrupt data (Tapani Raiko's dequantization)\n",
    "    y = x * 255.0\n",
    "    corruption_level = 1.0\n",
    "    y = y + corruption_level * tf.random_uniform(xs)\n",
    "    y = y / (255.0 + corruption_level)\n",
    "\n",
    "    jac = 0\n",
    "    y = y * (1 - 2 * alpha) + alpha\n",
    "    if model_type == \"nice\":\n",
    "        jac = tf.reduce_sum(-tf.log(y) - tf.log(1 - y) + tf.log(1 - 2 * alpha), [1]) \n",
    "    else:\n",
    "        jac = tf.reduce_sum(-tf.log(y) - tf.log(1 - y) + tf.log(1 - 2 * alpha), [1, 2, 3])\n",
    "    y = tf.log(y) - tf.log(1 - y)\n",
    "    sum_log_det_jacobians += jac\n",
    "\n",
    "    if len(layers) == 0:\n",
    "        if model_type == \"nice\":\n",
    "            construct_nice_spec(init_type=init_type, hidden_layers=hidden_layers, no_of_layers=no_of_layers)\n",
    "        else:\n",
    "            construct_model_spec(no_of_layers=no_of_layers, add_scaling=(batch_norm_adaptive != 0))\n",
    "\n",
    "    z = None\n",
    "    jac = sum_log_det_jacobians\n",
    "    for layer in layers:\n",
    "        y, jac, z = layer.forward_and_jacobian(y, jac, z, reuse=reuse, train=train)\n",
    "\n",
    "    if model_type == \"nice\":\n",
    "        z = y\n",
    "    else:\n",
    "        z = tf.concat(axis=3, values=[z, y])\n",
    "\n",
    "    final_latent_dimension = int_shape(z)\n",
    "\n",
    "    return z, jac\n",
    "\n",
    "\n",
    "def inv_model_spec(y, reuse=False, model_type=\"nice\", train=False, alpha=1e-7):\n",
    "  # construct inverse pass for sampling\n",
    "  if model_type == \"nice\":\n",
    "    z = y\n",
    "  else:\n",
    "    shape = final_latent_dimension\n",
    "    z = tf.reshape(y, [-1, shape[1], shape[2], shape[3]])\n",
    "    y = None\n",
    "\n",
    "  for layer in reversed(layers):\n",
    "    y,z = layer.backward(y,z, reuse=reuse, train=train)\n",
    "\n",
    "  # inverse logit\n",
    "  x = y\n",
    "\n",
    "  x = tf.sigmoid(y)\n",
    "  x = (x-alpha)/(1-2*alpha)\n",
    "  return x\n",
    "    \n",
    "\n",
    "# Computes log_likelihood of the network\n",
    "def log_likelihood(z, sum_log_det_jacobians, prior):\n",
    "  return -tf.reduce_sum(compute_log_density_x(z, sum_log_det_jacobians, prior))\n",
    "\n",
    "def int_shape(x):\n",
    "    return list(map(int, x.get_shape()))\n",
    "\n",
    "# Abstract class that can propagate both forward/backward,\n",
    "# along with jacobians.\n",
    "class Layer():\n",
    "  def __init__(self, mask_type, name='Coupling'):\n",
    "    tf.set_random_seed(0)\n",
    "    np.random.seed(0)\n",
    "  \n",
    "  def forward_and_jacobian(self, x, sum_log_det_jacobians, z):\n",
    "    raise NotImplementedError(str(type(self)))\n",
    "\n",
    "  def backward(self, y, z):\n",
    "    raise NotImplementedError(str(type(self)))\n",
    "\n",
    "def batch_norm(input_,\n",
    "                name,\n",
    "                train=True,\n",
    "                epsilon=1e-6, \n",
    "                decay=.1,\n",
    "                axes=[0, 1],\n",
    "                reuse=None,\n",
    "                bn_lag=0.,\n",
    "                dim=[],\n",
    "                scaling = True):\n",
    "  \"\"\"Batch normalization with corresponding log determinant Jacobian.\"\"\"\n",
    "  if reuse is None:\n",
    "      reuse = not train\n",
    "  # create variables\n",
    "  with tf.variable_scope(name) as scope:\n",
    "      if reuse:\n",
    "          scope.reuse_variables()\n",
    "      var = tf.get_variable(\n",
    "          \"var\", dim, tf.float32, tf.constant_initializer(1.), trainable=False)\n",
    "      mean = tf.get_variable(\n",
    "          \"mean\", dim, tf.float32, tf.constant_initializer(0.), trainable=False)\n",
    "      step = tf.get_variable(\"step\", [], tf.float32, tf.constant_initializer(0.), trainable=False)\n",
    "      if scaling:\n",
    "        scale_g = tf.get_variable(\"g_scale\", dim, tf.float32, tf.constant_initializer(1.))\n",
    "        shift_b = tf.get_variable(\"g_shift\", dim, tf.float32, tf.constant_initializer(0.))\n",
    "  # choose the appropriate moments\n",
    "  if train:\n",
    "      used_mean, used_var = tf.moments(input_, axes, name=\"batch_norm\")\n",
    "      cur_mean, cur_var = used_mean, used_var\n",
    "      if bn_lag > 0.:\n",
    "          used_var = stable_var(input_=input_, mean=used_mean, axes=axes)\n",
    "          cur_var = used_var\n",
    "          used_mean -= (1 - bn_lag) * (used_mean - tf.stop_gradient(mean))\n",
    "          used_mean /= (1. - bn_lag**(step + 1))\n",
    "          used_var -= (1 - bn_lag) * (used_var - tf.stop_gradient(var))\n",
    "          used_var /= (1. - bn_lag**(step + 1))\n",
    "  else:\n",
    "      used_mean, used_var = mean, var\n",
    "      cur_mean, cur_var = used_mean, used_var\n",
    "\n",
    "  # update variables\n",
    "  if train:\n",
    "      with tf.name_scope(name, \"AssignMovingAvg\", [mean, cur_mean, decay]):\n",
    "          with ops.colocate_with(mean):\n",
    "              new_mean = tf.assign_sub(\n",
    "                  mean,\n",
    "                  tf.check_numerics(\n",
    "                      decay * (mean - cur_mean), \"NaN in moving mean.\"))\n",
    "      with tf.name_scope(name, \"AssignMovingAvg\", [var, cur_var, decay]):\n",
    "          with ops.colocate_with(var):\n",
    "              new_var = tf.assign_sub(\n",
    "                  var,\n",
    "                  tf.check_numerics(decay * (var - cur_var),\n",
    "                                    \"NaN in moving variance.\"))\n",
    "      with tf.name_scope(name, \"IncrementTime\", [step]):\n",
    "          with ops.colocate_with(step):\n",
    "              new_step = tf.assign_add(step, 1.)\n",
    "      used_var += 0. * new_mean * new_var * new_step\n",
    "  used_var += epsilon\n",
    "  if scaling:\n",
    "    return ((input_- used_mean)/tf.sqrt(used_var)) * scale_g + shift_b\n",
    "  else:\n",
    "    return ((input_- used_mean)/tf.sqrt(used_var))\n",
    "\n",
    "def get_weight(name, weights_shape, init_type):\n",
    "  if init_type == \"uniform\":\n",
    "    weights = tf.get_variable(name, weights_shape, tf.float32,\n",
    "      tf.random_uniform_initializer(-0.01, 0.01, seed=0))\n",
    "  elif init_type == \"normal\":\n",
    "    weights = tf.get_variable(name, weights_shape, tf.float32,\n",
    "      tf.truncated_normal_initializer(stddev=0.02, seed =0))\n",
    "  elif init_type == \"orthogonal\":\n",
    "    weights = tf.get_variable(name, weights_shape, tf.float32,\n",
    "      tf.orthogonal_initializer(seed=0))\n",
    "  elif init_type == \"xavier\":\n",
    "    weights = tf.get_variable(name, weights_shape, tf.float32,\n",
    "      tf.keras.initializers.GlorotUniform(seed=0))\n",
    "  return weights\n",
    "\n",
    "#The NICE coupling layer\n",
    "#Only for MNIST and checkerboard masking is done be rordering\n",
    "class NICECouplingLayer(Layer):\n",
    "\n",
    "  # |mask_type| can be 'checkerboard0', 'checkerboard1', 'channel0', 'channel1'\n",
    "  def __init__(self, mask_type, name='Coupling', seed=0, init_type= \"uniform\",\n",
    "    hidden_layers=1000, no_of_layers=1):\n",
    "    self.mask_type = mask_type\n",
    "    self.name = name\n",
    "    self.seed = seed\n",
    "    self.init_type = init_type\n",
    "    self.hidden_states = hidden_layers\n",
    "    self.no_of_layers = no_of_layers\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "  # performs the operation described in the NICE paper\n",
    "  def function_l_m(self,x,mask,name='function_l_m', reuse=False, train=False):\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "      hidden_states = self.hidden_states\n",
    "      inp_shape = int((x.get_shape())[1])\n",
    "\n",
    "      y = x\n",
    "      #l1      \n",
    "      bn = batch_norm(name=\"g_bn_in\" % r, epsilon=1e-4)\n",
    "      y = bn.__call__(y, train=train)\n",
    "      weights_shape = [inp_shape, hidden_states]\n",
    "      weights = get_weight(\"g_weights_input\", weights_shape, self.init_type)\n",
    "      bias = tf.get_variable(\"g_bias_input\", [hidden_states], tf.float32, tf.constant_initializer(0.0))\n",
    "      y = tf.nn.relu(tf.matmul(y, weights) + bias)\n",
    "\n",
    "      for r in range(self.no_of_layers):\n",
    "        bn = batch_norm(name=\"g_bn_%d\" % r, epsilon=1e-4)\n",
    "        y = bn.__call__(y, train=train)\n",
    "\n",
    "        weights_shape = [hidden_states, hidden_states]\n",
    "  \n",
    "        weights = get_weight(\"g_weights_%d\" % r, weights_shape, self.init_type)\n",
    "  \n",
    "        bias = tf.get_variable(\"g_bias_%d\" % r, [hidden_states], tf.float32, tf.constant_initializer(0.0))\n",
    "        y = tf.nn.relu(tf.matmul(y, weights) + bias)\n",
    "      \n",
    "      #lout\n",
    "      bn = batch_norm(name=\"g_bn_out\" % r, epsilon=1e-4)\n",
    "      y = bn.__call__(y, train=train) \n",
    "      weights_shape = [hidden_states, inp_shape]\n",
    "      weights = get_weight(\"g_weights_out\", weights_shape, self.init_type)\n",
    "      bias = tf.get_variable(\"g_bias_out\", [inp_shape], tf.float32, tf.constant_initializer(0.0))\n",
    "      y = tf.matmul(y, weights) + bias\n",
    "\n",
    "      return y\n",
    "\n",
    "  # corresponds to the coupling layer of the NICE paper\n",
    "  def forward_and_jacobian(self, x, sum_log_det_jacobians, z, reuse, train):\n",
    "    tf.set_random_seed(self.seed)\n",
    "    np.random.seed(self.seed) \n",
    "    with tf.variable_scope(self.name, reuse=reuse):\n",
    "      \n",
    "      split_value = int(int((x.get_shape())[1])/2)\n",
    "      \n",
    "      # masked half of x\n",
    "      y = x\n",
    "      x1 = x[:,:split_value]\n",
    "      x2 = x[:,split_value:]\n",
    "      if self.mask_type == 'checkerboard0':\n",
    "        mx1 = self.function_l_m(x1, reuse=reuse, train=train)\n",
    "        delta = tf.concat([tf.zeros_like(x1),mx1], axis=1)\n",
    "      else:\n",
    "        mx2 = self.function_l_m(x2, reuse=reuse, train=train)\n",
    "        delta = tf.concat([mx2, tf.zeros_like(x2)], axis=1)\n",
    "      y += delta\n",
    "      \n",
    "      return y, sum_log_det_jacobians, z\n",
    "\n",
    "  def backward(self, y, z, reuse, train):\n",
    "    tf.set_random_seed(self.seed)\n",
    "    np.random.seed(self.seed) \n",
    "    with tf.variable_scope(self.name, reuse=reuse):\n",
    "      \n",
    "      split_value = int(int((y.get_shape())[1])/2)\n",
    "      \n",
    "      # masked half of x\n",
    "      x = y\n",
    "      y1 = y[:,:split_value]\n",
    "      y2 = y[:,split_value:]\n",
    "      if self.mask_type == 'checkerboard0':\n",
    "        my1 = self.function_l_m(y1, reuse=reuse, train=train)\n",
    "        delta = tf.concat([tf.zeros_like(y1), my1], axis=1)\n",
    "      else:\n",
    "        my2 = self.function_l_m(y2, reuse=reuse, train=train)\n",
    "        delta = tf.concat([my2, tf.zeros_like(y2)], axis=1)\n",
    "      x -= delta\n",
    "      \n",
    "      return x, z\n",
    "\n",
    "#The NICE coupling layer\n",
    "class NICEScaling(Layer):\n",
    "\n",
    "  def __init__(self, name='scaling', seed=0):\n",
    "    self.name = name  \n",
    "    self.seed = 0\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "  # Split elements in a checkerboard fashion\n",
    "  def forward_and_jacobian(self, x, sum_log_det_jacobians, z, reuse, train=None):\n",
    "    tf.set_random_seed(self.seed)\n",
    "    np.random.seed(self.seed) \n",
    "    with tf.variable_scope(self.name, reuse=reuse):\n",
    "      \n",
    "      no_of_factors = int((x.get_shape())[1]) #xs[1]\n",
    "      scale_factor = tf.get_variable(\"g_scale\", [1, no_of_factors], tf.float32, tf.constant_initializer(0.0))\n",
    "      \n",
    "      y = tf.multiply(x, tf.exp(scale_factor)) \n",
    "\n",
    "      return y, sum_log_det_jacobians + tf.reduce_sum(scale_factor), z\n",
    "\n",
    "  def backward(self, y, z,reuse, train=None):\n",
    "    tf.set_random_seed(0)\n",
    "    np.random.seed(0) \n",
    "    with tf.variable_scope(self.name, reuse=reuse):\n",
    "      \n",
    "      no_of_factors = int((y.get_shape())[1]) #ys[1]\n",
    "      scale_factor = tf.get_variable(\"g_scale\", [1, no_of_factors], tf.float32, tf.constant_initializer(0.0))\n",
    "      \n",
    "      x = tf.multiply(y, tf.exp(-scale_factor)) \n",
    "\n",
    "      return x, z\n",
    "\n",
    "def simple_batch_norm(x):\n",
    "    mu = tf.reduce_mean(x)\n",
    "    sig2 = tf.reduce_mean(tf.square(x-mu))    \n",
    "    x = (x-mu)/tf.sqrt(sig2 + 1.0e-6)\n",
    "    return x\n",
    "\n",
    "# The coupling layer.\n",
    "# Contains code for both checkerboard and channelwise masking.\n",
    "class CouplingLayer(Layer):\n",
    "\n",
    "  # |mask_type| can be 'checkerboard0', 'checkerboard1', 'channel0', 'channel1'\n",
    "  def __init__(self, mask_type, name='Coupling', num_residual_blocks=8, scaling=True):\n",
    "    self.mask_type = mask_type\n",
    "    self.name = name\n",
    "    self.num_residual_blocks = num_residual_blocks\n",
    "    self.scaling = scaling\n",
    "    if self.scaling == False:\n",
    "      print(\"No scaling\")\n",
    "\n",
    "    tf.set_random_seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "  # Weight normalization technique\n",
    "  def get_normalized_weights(self, name, weights_shape):\n",
    "        weights = tf.get_variable(name, weights_shape, tf.float32,\n",
    "                                  tf.glorot_uniform_initializer(seed=0))\n",
    "        scale = tf.get_variable(name + \"_scale\", [weights_shape[-1]], tf.float32, \n",
    "                                tf.glorot_uniform_initializer(seed=0),\n",
    "                                regularizer=tf.keras.regularizers.l2(5e-5))\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(weights), [0, 1, 2]))\n",
    "        return weights / norm * scale\n",
    "\n",
    "    \n",
    "  \n",
    "  # corresponds to the function m and l in the RealNVP paper\n",
    "  # (Function m and l became s and t in the new version of the paper)\n",
    "  def function_l_m(self,x,mask,name='function_l_m', reuse=False, train=False):\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "      channel = 64\n",
    "      padding = 'SAME'\n",
    "      xs = int_shape(x)\n",
    "      kernel_h = 3\n",
    "      kernel_w = 3\n",
    "      input_channel = xs[3]\n",
    "      y = x\n",
    "\n",
    "      # y = batch_norm(input_=y, name=\"g_bn_in1\", train=train, scale=False)\n",
    "      if not self.scaling:\n",
    "        y = simple_batch_norm(y)\n",
    "      else:\n",
    "        bn = batch_norm(epsilon=1e-4, name=\"g_bn_in\")\n",
    "        y = bn(y, train=train)\n",
    "      weights_shape = [1, 1, input_channel, channel]\n",
    "      weights = self.get_normalized_weights(\"g_weights_input\", weights_shape)\n",
    "      \n",
    "      y = tf.nn.conv2d(y, weights, [1, 1, 1, 1], padding=padding)\n",
    "      if not self.scaling:\n",
    "        print(\"this\")\n",
    "        y = simple_batch_norm(y)\n",
    "        # bn = batch_norm(name=\"g_bn_in2\", epsilon=1e-4)\n",
    "        # y = bn(y, train=train)\n",
    "\n",
    "        #   train=train, epsilon=1e-4, axes=[0,1,2], reuse=reuse, scaling=False)\n",
    "      else:\n",
    "        biases = tf.get_variable('g_biases_input', [channel], initializer=tf.constant_initializer(0.0))\n",
    "        y = tf.reshape(tf.nn.bias_add(y, biases), y.get_shape())\n",
    "      y = tf.nn.relu(y)\n",
    "      if self.scaling:\n",
    "        bn = batch_norm(name=\"g_bn_in2\", epsilon=1e-4)\n",
    "        y = bn(y, train=train)\n",
    "\n",
    "      skip = y\n",
    "      # Residual blocks\n",
    "      num_residual_blocks = self.num_residual_blocks\n",
    "      for r in range(num_residual_blocks):\n",
    "        weights_shape = [kernel_h, kernel_w, channel, channel]\n",
    "        weights = self.get_normalized_weights(\"g_weights%d_1\" % r, weights_shape)\n",
    "        y = tf.nn.conv2d(y, weights, [1, 1, 1, 1], padding=padding)\n",
    "        if not self.scaling:\n",
    "          y = simple_batch_norm(y)\n",
    "          # y = batch_norm(input_=y, dim=channel, name=\"g_bn%d_1\" % r,\n",
    "          #   train=train, epsilon=1e-4, axes=[0,1,2], reuse=reuse, scaling=False)\n",
    "        else:\n",
    "          biases = tf.get_variable('g_biases_%d_1' % r, [channel], initializer=tf.constant_initializer(0.0))\n",
    "          y = tf.reshape(tf.nn.bias_add(y, biases), y.get_shape())\n",
    "        y = tf.nn.relu(y)\n",
    "        if self.scaling:\n",
    "          bn = batch_norm(name=\"g_bn%d_1\" % r, epsilon=1e-4)\n",
    "          y = bn(y, train=train)\n",
    "        \n",
    "        weights_shape = [kernel_h, kernel_w, channel, channel]\n",
    "        weights = self.get_normalized_weights(\"g_weights%d_2\" % r, weights_shape)\n",
    "        y = tf.nn.conv2d(y, weights, [1, 1, 1, 1], padding=padding)\n",
    "\n",
    "        if not self.scaling:\n",
    "          y = simple_batch_norm(y)\n",
    "          # y = batch_norm(input_=y, dim=channel, name=\"g_bn%d_2\" % r,\n",
    "          #   train=train, epsilon=1e-4, axes=[0,1,2], reuse=reuse, scaling=False)\n",
    "        else:\n",
    "          biases = tf.get_variable('g_biases_%d_2' % r, [channel], initializer=tf.constant_initializer(0.0))\n",
    "          y = tf.reshape(tf.nn.bias_add(y, biases), y.get_shape())\n",
    "\n",
    "        y += skip\n",
    "        y = tf.nn.relu(y)\n",
    "        if self.scaling:\n",
    "            bn = batch_norm(name=\"g_bn%d_2\" % r, epsilon=1e-4)\n",
    "            y = bn(y, train=train)\n",
    "\n",
    "        skip = y\n",
    "\n",
    "        \n",
    "      # 1x1 convolution for reducing dimension\n",
    "      weights = self.get_normalized_weights(\"g_weights_output\", \n",
    "                                            [1, 1, channel, input_channel*2])\n",
    "      y = tf.nn.conv2d(y, weights, [1, 1, 1, 1], padding=padding)    \n",
    "      biases = tf.get_variable('g_biases_output', [input_channel*2], initializer=tf.constant_initializer(0.0))\n",
    "      y = tf.reshape(tf.nn.bias_add(y, biases), y.get_shape())\n",
    "      # For numerical stability, apply tanh and then scale\n",
    "      y = tf.tanh(y)\n",
    "      \n",
    "      if 'checkerboard' in self.mask_type:\n",
    "        scale_factor = tf.get_variable(\"g_weights_tanh_scale\", [1], tf.float32,\n",
    "            initializer=tf.constant_initializer(0.),\n",
    "            regularizer=regularizers.l2(5e-5))\n",
    "      else:\n",
    "        scale_factor = tf.get_variable(\"g_weights_tanh_scale\", [1], tf.float32, \\\n",
    "          tf.constant_initializer(1.))\n",
    "      scale_shift = tf.get_variable(\"g_weights_scale_shift\", [1], tf.float32, \\\n",
    "          tf.constant_initializer(0.))\n",
    "      \n",
    "      \n",
    "\n",
    "      # The first half defines the l function\n",
    "      # The second half defines the m function\n",
    "      l = (y[:,:,:,:input_channel] * scale_factor + scale_shift) * (-mask+1)\n",
    "      m = y[:,:,:,input_channel:] * (-mask+1)\n",
    "\n",
    "      return l,m\n",
    "\n",
    "\n",
    "  \n",
    "  # returns constant tensor of masks\n",
    "  # |xs| is the size of tensor\n",
    "  # |mask_type| can be 'checkerboard0', 'checkerboard1', 'channel0', 'channel1'\n",
    "  # |b| has the dimension of |xs|\n",
    "  def get_mask(self, x, mask_type):\n",
    "    shape = tf.shape(x)\n",
    "    batch_size = shape[0]\n",
    "    height = shape[1]\n",
    "    width = shape[2]\n",
    "    channels = shape[3]\n",
    "\n",
    "    even_height = height - tf.math.mod(height, 2)\n",
    "    even_width = width - tf.math.mod(width, 2)\n",
    "\n",
    "    if 'checkerboard' in mask_type:\n",
    "        unit0 = tf.constant([[0.0, 1.0], [1.0, 0.0]], dtype=tf.float32)\n",
    "        unit1 = 1.0 - unit0\n",
    "        unit = unit0 if mask_type == 'checkerboard0' else unit1\n",
    "        unit = tf.reshape(unit, [1, 2, 2, 1])\n",
    "\n",
    "        b = tf.tile(unit, [batch_size, even_height // 2, even_width // 2, channels])\n",
    "\n",
    "        # Pad mask back to original height and width\n",
    "        pad_h = height - even_height\n",
    "        pad_w = width - even_width\n",
    "        paddings = [[0, 0], [0, pad_h], [0, pad_w], [0, 0]]\n",
    "        b = tf.pad(b, paddings, \"CONSTANT\")\n",
    "        \n",
    "    elif 'channel' in mask_type:\n",
    "        half_channels = tf.cast(channels // 2, tf.int32)\n",
    "        full_shape = tf.stack([batch_size, height, width, half_channels])\n",
    "        white = tf.ones(full_shape, dtype=tf.float32)\n",
    "        black = tf.zeros(full_shape, dtype=tf.float32)\n",
    "        b = tf.concat([white, black], axis=3) if mask_type == 'channel0' else tf.concat([black, white], axis=3)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mask_type {mask_type}\")\n",
    "\n",
    "    return b\n",
    "\n",
    "\n",
    "  # corresponds to the coupling layer of the RealNVP paper\n",
    "  # |mask_type| can be 'checkerboard0', 'checkerboard1', 'channel0', 'channel1'\n",
    "  # log_det_jacobian is a 1D tensor of size (batch_size)\n",
    "  def forward_and_jacobian(self, x, sum_log_det_jacobians, z, reuse=False, train=False):\n",
    "    with tf.variable_scope(self.name, reuse=reuse):\n",
    "      b = self.get_mask(x, self.mask_type)\n",
    "\n",
    "\n",
    "      # masked half of x\n",
    "      x1 = x * b\n",
    "      l,m = self.function_l_m(x1, b, reuse=reuse, train=train)\n",
    "      l = tf.clip_by_value(l, -5.0, 5.0)  # Avoid large exponentials\n",
    "      y = x1 + tf.multiply(-b+1.0, x * tf.exp(l) + m)\n",
    "      log_det_jacobian = tf.reduce_sum(l, [1,2,3])\n",
    "      sum_log_det_jacobians += log_det_jacobian\n",
    "\n",
    "      return y,sum_log_det_jacobians, z\n",
    "\n",
    "  def backward(self, y, z, reuse=False, train=False):    \n",
    "    with tf.variable_scope(self.name, reuse=True):\n",
    "        b = self.get_mask(y, self.mask_type)  # <- FIXED HERE\n",
    "\n",
    "        y1 = y * b\n",
    "        l, m = self.function_l_m(y1, b, reuse=reuse, train=train)\n",
    "        x = y1 + tf.multiply(y * (-b + 1.0) - m, tf.check_numerics(tf.exp(-l), \"exp has NaN\"))\n",
    "        return x, z\n",
    "\n",
    "\n",
    "# The layer that performs squeezing.\n",
    "# Only changes the dimension.\n",
    "# The Jacobian is untouched and just passed to the next layer\n",
    "class SqueezingLayer(Layer):\n",
    "  def __init__(self, name=\"Squeeze\"):\n",
    "    self.name = name\n",
    "\n",
    "  def forward_and_jacobian(self, x, sum_log_det_jacobians, z, reuse=False, train=False):\n",
    "    xs = int_shape(x)\n",
    "    assert xs[1] % 2 == 0 and xs[2] % 2 == 0\n",
    "    y = tf.space_to_depth(x, 2)\n",
    "    if z is not None:\n",
    "      z = tf.space_to_depth(z, 2)      \n",
    "\n",
    "    return y,sum_log_det_jacobians, z\n",
    "\n",
    "  def backward(self, y, z, reuse=False, train=False):\n",
    "    ys = int_shape(y)\n",
    "    assert ys[3] % 4 == 0\n",
    "    x = tf.depth_to_space(y,2)\n",
    "\n",
    "    if z is not None:\n",
    "      z = tf.depth_to_space(z,2)\n",
    "\n",
    "    return x, z\n",
    "\n",
    "# The layer that factors out half of the variables\n",
    "# directly to the latent space.  \n",
    "class FactorOutLayer(Layer):\n",
    "  def __init__(self, scale, name='FactorOut'):\n",
    "    self.scale = scale\n",
    "    self.name = name\n",
    "  \n",
    "  def forward_and_jacobian(self, x, sum_log_det_jacobians, z, reuse=False, train=False):\n",
    "\n",
    "    xs = int_shape(x)\n",
    "    split = xs[3]//2\n",
    "\n",
    "    # The factoring out is done on the channel direction.\n",
    "    # Haven't experimented with other ways of factoring out.\n",
    "    new_z = x[:,:,:,:split]\n",
    "    x = x[:,:,:,split:]\n",
    "\n",
    "    if z is not None:\n",
    "      z = tf.concat(axis=3, values=[z, new_z])\n",
    "    else:\n",
    "      z = new_z\n",
    "    \n",
    "    return x, sum_log_det_jacobians, z\n",
    "  \n",
    "  def backward(self, y, z, reuse=False, train=False):\n",
    "\n",
    "    # At scale 0, 1/2 of the original dimensions are factored out\n",
    "    # At scale 1, 1/4 of the original dimensions are factored out\n",
    "    # ....\n",
    "    # At scale s, (1/2)^(s+1) are factored out\n",
    "    # Hence, at backward pass of scale s, (1/2)^(s) of z should be factored in\n",
    "    \n",
    "    zs = int_shape(z)\n",
    "    if y is None:\n",
    "      split = zs[3] // (2**self.scale)\n",
    "    else:\n",
    "      split = int_shape(y)[3]\n",
    "    new_y = z[:,:,:,-split:]\n",
    "    z = z[:,:,:,:-split]\n",
    "\n",
    "    assert (int_shape(new_y)[3] == split)\n",
    "\n",
    "    if y is not None:\n",
    "      x = tf.concat(axis=3, values=[new_y, y])\n",
    "    else:\n",
    "      x = new_y\n",
    "\n",
    "    return x, z\n",
    "\n",
    "\n",
    "# Given the output of the network and all jacobians, \n",
    "# compute the log probability.\n",
    "def compute_log_density_x(z, sum_log_det_jacobians, prior):\n",
    "\n",
    "  zs = int_shape(z)\n",
    "  if len(zs) == 4:\n",
    "    K = zs[1]*zs[2]*zs[3] #dimension of the Gaussian distribution\n",
    "    z = tf.reshape(z, (-1, K))\n",
    "  else:\n",
    "    K = zs[1]\n",
    "  if prior == \"gaussian\":\n",
    "    log_density_z = -0.5*tf.reduce_sum(tf.square(z), [1]) -0.5*K*np.log(2*np.pi)\n",
    "  elif prior == \"logistic\":\n",
    "    log_density_z = -tf.reduce_sum(-z + 2*tf.softplus(z),[1])\n",
    "  elif prior == \"uniform\":\n",
    "    log_density_z = 0\n",
    "  log_density_x = log_density_z + sum_log_det_jacobians\n",
    "\n",
    "  return log_density_x\n",
    "\n",
    "\n",
    "\n",
    "class DCGAN(object):\n",
    "  def __init__(self, sess, input_height=32, input_width=32,\n",
    "         batch_size=64, sample_num = 64, z_dim=100, gf_dim=64, df_dim=64,\n",
    "         gfc_dim=1024, dfc_dim=1024, c_dim=3, dataset_name='default', checkpoint_dir=None,\n",
    "         f_div='cross-ent', prior=\"logistic\", min_lr=0.0, lr_decay=1.0,\n",
    "         model_type=\"nice\", alpha=1e-7, log_dir=None,\n",
    "         init_type=\"uniform\",reg=0.5, n_critic=1.0, hidden_layers=1000,\n",
    "         no_of_layers= 8, like_reg=0.1, just_sample=False, batch_norm_adaptive=1, flags = None):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "      sess: TensorFlow session\n",
    "      batch_size: The size of batch. Should be specified before training.\n",
    "      y_dim: (optional) Dimension of dim for y. [None]\n",
    "      z_dim: (optional) Dimension of dim for Z. [100]\n",
    "      gf_dim: (optional) Dimension of gen filters in first conv layer. [64]\n",
    "      df_dim: (optional) Dimension of discrim filters in first conv layer. [64]\n",
    "      gfc_dim: (optional) Dimension of gen units for for fully connected layer. [1024]\n",
    "      dfc_dim: (optional) Dimension of discrim units for fully connected layer. [1024]\n",
    "      c_dim: (optional) Dimension of image color. For grayscale input, set to 1. [3]\n",
    "    \"\"\"\n",
    "    self.sess = sess\n",
    "    self.is_grayscale = (c_dim == 1)\n",
    "\n",
    "    self.batch_size = batch_size\n",
    "    self.sample_num = batch_size\n",
    "    \n",
    "    self.input_height = input_height\n",
    "    self.input_width = input_width\n",
    "    self.prior = prior\n",
    "\n",
    "    self.z_dim = z_dim\n",
    "    self.flags = flags \n",
    "\n",
    "    self.gf_dim = gf_dim\n",
    "    self.df_dim = df_dim\n",
    "\n",
    "    self.gfc_dim = gfc_dim\n",
    "    self.dfc_dim = dfc_dim\n",
    "\n",
    "    self.c_dim = c_dim\n",
    "\n",
    "    self.lr_decay = lr_decay\n",
    "    self.min_lr = min_lr\n",
    "    self.model_type = model_type\n",
    "    self.log_dir = log_dir\n",
    "    self.alpha = alpha\n",
    "    self.init_type = init_type\n",
    "    self.reg = reg\n",
    "    self.n_critic = n_critic\n",
    "    self.hidden_layers = hidden_layers\n",
    "    self.no_of_layers = no_of_layers\n",
    "    \n",
    "    # batch normalization : deals with poor initialization helps gradient flow\n",
    "    self.d_bn1 = batch_norm(name='d_bn1')\n",
    "    self.d_bn2 = batch_norm(name='d_bn2')\n",
    "    self.dataset_name = dataset_name\n",
    "    self.like_reg = like_reg\n",
    "    if self.dataset_name != 'mnist':\n",
    "      self.d_bn3 = batch_norm(name='d_bn3')\n",
    "\n",
    "    self.checkpoint_dir = checkpoint_dir\n",
    "    self.f_div = f_div\n",
    "    \n",
    "    seed = 0\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "    \n",
    "    self.build_model()\n",
    "\n",
    "  def build_model(self):\n",
    "    seed =0\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "\n",
    "    image_dims = [self.input_height, self.input_width, self.c_dim]\n",
    "\n",
    "    self.inputs = tf.placeholder(\n",
    "      tf.float32, [self.batch_size] + image_dims, name='real_images')\n",
    "    self.sample_inputs = tf.placeholder(\n",
    "      tf.float32, [self.sample_num] + image_dims, name='sample_inputs')\n",
    "    self.image_size = np.prod(image_dims)\n",
    "    self.image_dims = image_dims\n",
    "    if self.dataset_name == \"cifar\":\n",
    "      inputs = tf.map_fn(lambda img: tf.image.random_flip_left_right(img), self.inputs)\n",
    "    else:\n",
    "      inputs = self.inputs\n",
    "\n",
    "    sample_inputs = self.sample_inputs\n",
    "\n",
    "    self.z = tf.placeholder(\n",
    "      tf.float32, [self.batch_size, self.z_dim], name='z')\n",
    "    self.z_sum = histogram_summary(\"z\", self.z)\n",
    "\n",
    "    #### f: Image Space to Latent space #########\n",
    "    batch_norm_adaptive = self.flags.batch_norm_adaptive\n",
    "    # Forward flow model: shared template for both training and inference\n",
    "    self.flow_model_template = tf.make_template(\n",
    "        'flow_model',\n",
    "        lambda x, train: model_spec(\n",
    "            x,\n",
    "            reuse=tf.AUTO_REUSE,\n",
    "            model_type=self.model_type,\n",
    "            train=train,\n",
    "            alpha=self.alpha,\n",
    "            init_type=self.init_type,\n",
    "            hidden_layers=self.hidden_layers,\n",
    "            no_of_layers=self.no_of_layers,\n",
    "            batch_norm_adaptive=self.flags.batch_norm_adaptive\n",
    "        )\n",
    "    )\n",
    "    # self.flow_model = tf.make_template('model', \n",
    "    #   lambda x: model_spec(x, reuse=False, model_type=self.model_type, train=False, \n",
    "    #     alpha=self.alpha, init_type=self.init_type, hidden_layers=self.hidden_layers,\n",
    "    #     no_of_layers=self.no_of_layers, batch_norm_adaptive=batch_norm_adaptive), unique_name_='model')\n",
    "\n",
    "    #### f: Image Space to Latent space for training #########\n",
    "    def trainable_flow_model_fn(x):\n",
    "        return model_spec(\n",
    "            x,\n",
    "            reuse=tf.AUTO_REUSE,\n",
    "            model_type=self.model_type,\n",
    "            train=True,\n",
    "            alpha=self.alpha,\n",
    "            init_type=self.init_type,\n",
    "            hidden_layers=self.hidden_layers,\n",
    "            no_of_layers=self.no_of_layers,\n",
    "            batch_norm_adaptive=self.flags.batch_norm_adaptive\n",
    "        )\n",
    "\n",
    "\n",
    "    self.trainable_flow_model = trainable_flow_model_fn\n",
    "\n",
    "    # Use flow_model_template for both:\n",
    "    self.flow_model = lambda x: self.flow_model_template(x, train=False)\n",
    "    self.trainable_flow_model = lambda x: self.flow_model_template(x, train=True)\n",
    "\n",
    "\n",
    "    # ##### f^-1: Latent to image (trainable)#######\n",
    "    \n",
    "    # self.flow_inv_model = tf.make_template('model', \n",
    "    #   lambda x: inv_model_spec(x, reuse=True, model_type=self.model_type,\n",
    "    #    train=True,alpha=self.alpha), unique_name_='model')\n",
    "    # Inverse flow model (used for generation)\n",
    "    self.flow_inv_template = tf.make_template(\n",
    "        'flow_inv_model',\n",
    "        lambda z, train: inv_model_spec(\n",
    "            z,\n",
    "            reuse=tf.AUTO_REUSE,\n",
    "            model_type=self.model_type,\n",
    "            train=train,\n",
    "            alpha=self.alpha\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # ##### f^-1: Latent to image (not-trainable just for sampling)#######\n",
    "    self.sampler_function = tf.make_template('model', \n",
    "      lambda x: inv_model_spec(x, reuse=True, model_type=self.model_type, \n",
    "        alpha=self.alpha,train=False), unique_name_='model')\n",
    "\n",
    "\n",
    "    self.flow_inv_model = lambda z: self.flow_inv_template(z, train=True)\n",
    "    self.sampler_function = lambda z: self.flow_inv_template(z, train=False)\n",
    "    self.generator_train_batch = self.flow_inv_model\n",
    "    \n",
    "    ############### SET SIZE FOR TEST BATCH DEPENDING ON WHETHER WE USE Linear or Conv arch##########\n",
    "    if self.model_type == \"nice\":\n",
    "      self.log_like_batch = tf.placeholder(\\\n",
    "        tf.float32, [self.batch_size, self.image_size], name='log_like_batch')\n",
    "    elif self.model_type == \"real_nvp\":\n",
    "      self.log_like_batch = tf.placeholder(\\\n",
    "        tf.float32, [self.batch_size] + self.image_dims, name='log_like_batch')\n",
    "    ###############################################\n",
    "\n",
    "    # Normalize input to prevent NaNs\n",
    "    mean, variance = tf.nn.moments(self.log_like_batch, axes=[0])\n",
    "    normalized_input = (self.log_like_batch - mean) / (tf.sqrt(variance) + 1e-8)\n",
    "\n",
    "    # Now use normalized_input instead\n",
    "    gen_para, jac = self.flow_model_template(normalized_input, train=False)\n",
    "\n",
    "    if self.dataset_name == \"mnist\":\n",
    "      self.log_likelihood = log_likelihood(gen_para, jac, self.prior)/(self.batch_size)\n",
    "    else:\n",
    "      # to calculate values in bits per dim we need to\n",
    "      # multiply the density by the width of the \n",
    "      # discrete probability area, which is 1/256.0, per dimension.\n",
    "      # The calculation is performed in the log space.\n",
    "      self.log_likelihood = log_likelihood(gen_para, jac, self.prior)/(self.batch_size)\n",
    "      self.log_likelihood = 8. + self.log_likelihood / (np.log(2)*self.image_size)\n",
    "\n",
    "    self.G_before_postprocessing = self.flow_inv_template(self.z, train=True)\n",
    "    self.sampler_before_postprocessing = self.flow_inv_template(self.z, train=False)\n",
    "\n",
    "\n",
    "    if self.model_type == \"real_nvp\":\n",
    "      ##For data dependent init (not completely implemented)\n",
    "      self.x_init = tf.placeholder(tf.float32, shape=[self.batch_size] + image_dims)\n",
    "      # run once for data dependent initialization of parameters\n",
    "      self.trainable_flow_model(self.x_init)\n",
    "    \n",
    "    inputs_tr_flow = inputs\n",
    "    if self.model_type == \"nice\":\n",
    "      split_val = int(self.image_size /2)\n",
    "      self.permutation = np.arange(self.image_size)\n",
    "      tmp = self.permutation.copy()\n",
    "      self.permutation[:split_val] = tmp[::2]\n",
    "      self.permutation[split_val:] = tmp[1::2]\n",
    "      self.for_perm = np.identity(self.image_size)\n",
    "      self.for_perm = tf.constant(self.for_perm[:,self.permutation], tf.float32)\n",
    "      self.rev_perm = np.identity(self.image_size)\n",
    "      self.rev_perm = tf.constant(self.rev_perm[:,np.argsort(self.permutation)], tf.float32)\n",
    "      self.G_before_postprocessing \\\n",
    "      = tf.matmul(self.G_before_postprocessing,self.rev_perm)\n",
    "      self.sampler_before_postprocessing \\\n",
    "      = tf.clip_by_value(tf.matmul(self.sampler_before_postprocessing, self.rev_perm) , 0., 1.)\n",
    "      inputs_tr_flow = tf.matmul(tf.reshape(inputs, [self.batch_size, self.image_size]), self.for_perm)\n",
    "\n",
    "    train_gen_para, train_jac = self.flow_model_template(inputs_tr_flow, train=True)\n",
    "    self.train_log_likelihood = log_likelihood(train_gen_para, train_jac, self.prior) / self.batch_size\n",
    "    \n",
    "    self.sampler = tf.reshape(self.sampler_before_postprocessing, [self.batch_size] + image_dims)\n",
    "    self.G = tf.reshape(self.G_before_postprocessing, [self.batch_size] + image_dims)\n",
    "\n",
    "    inputs = inputs*255.0\n",
    "    corruption_level = 1.0\n",
    "    inputs = inputs + corruption_level * tf.random_uniform([self.batch_size] + image_dims)\n",
    "    inputs = inputs/(255.0 + corruption_level)\n",
    "\n",
    "    self.D, self.D_logits = self.discriminator(inputs, reuse=False)\n",
    "\n",
    "    self.D_, self.D_logits_ = self.discriminator(self.G, reuse=True)\n",
    "\n",
    "    self.d_sum = histogram_summary(\"d\", self.D)\n",
    "    self.d__sum = histogram_summary(\"d_\", self.D_)\n",
    "    self.G_sum = image_summary(\"G\", self.G)\n",
    "\n",
    "    def sigmoid_cross_entropy_with_logits(x, y):\n",
    "      try:\n",
    "        return tf.sigmoid_cross_entropy_with_logits(logits=x, labels=y)\n",
    "      except:\n",
    "        return tf.sigmoid_cross_entropy_with_logits(logits=x, targets=y)\n",
    "\n",
    "    ### Vanilla gan loss\n",
    "    if self.f_div == 'ce':\n",
    "      self.d_loss_real = tf.reduce_mean(\n",
    "        sigmoid_cross_entropy_with_logits(self.D_logits, tf.ones_like(self.D)))\n",
    "      self.d_loss_fake = tf.reduce_mean(\n",
    "        sigmoid_cross_entropy_with_logits(self.D_logits_, tf.zeros_like(self.D_)))\n",
    "      self.g_loss = tf.reduce_mean(\n",
    "        sigmoid_cross_entropy_with_logits(self.D_logits_, tf.ones_like(self.D_)))\n",
    "    else:\n",
    "    ### other gan losses\n",
    "      if self.f_div == 'hellinger':\n",
    "        self.d_loss_real = tf.reduce_mean(tf.exp(-self.D_logits))\n",
    "        self.d_loss_fake = tf.reduce_mean(tf.exp(self.D_logits_) - 2.)\n",
    "        self.g_loss = tf.reduce_mean(tf.exp(-self.D_logits_))\n",
    "      elif self.f_div == 'rkl':\n",
    "        self.d_loss_real = tf.reduce_mean(tf.exp(self.D_logits))\n",
    "        self.d_loss_fake = tf.reduce_mean(-self.D_logits_ - 1.)\n",
    "        self.g_loss = -tf.reduce_mean(-self.D_logits_ - 1.)\n",
    "      elif self.f_div == 'kl':\n",
    "        self.d_loss_real = tf.reduce_mean(-self.D_logits)\n",
    "        self.d_loss_fake = tf.reduce_mean(tf.exp(self.D_logits_ - 1.))\n",
    "        self.g_loss = tf.reduce_mean(-self.D_logits_)\n",
    "      elif self.f_div == 'tv':\n",
    "        self.d_loss_real = tf.reduce_mean(-0.5 * tf.tanh(self.D_logits))\n",
    "        self.d_loss_fake = tf.reduce_mean(0.5 * tf.tanh(self.D_logits_))\n",
    "        self.g_loss = tf.reduce_mean(-0.5 * tf.tanh(self.D_logits_))\n",
    "      elif self.f_div == 'lsgan':\n",
    "        self.d_loss_real = 0.5 * tf.reduce_mean((self.D_logits-1)**2)\n",
    "        self.d_loss_fake = 0.5 * tf.reduce_mean(self.D_logits_**2)\n",
    "        self.g_loss = 0.5 * tf.reduce_mean((self.D_logits_-1)**2)\n",
    "      elif self.f_div == \"wgan\":\n",
    "        self.g_loss = -tf.reduce_mean(self.D_logits_)\n",
    "        self.d_loss_real = -tf.reduce_mean(self.D_logits)\n",
    "        self.d_loss_fake = tf.reduce_mean(self.D_logits_)\n",
    "        alpha = tf.random_uniform(\n",
    "            shape=[1, self.batch_size], \n",
    "            minval=0.,\n",
    "            maxval=1.\n",
    "        )\n",
    "        fake_data = self.G\n",
    "        real_data = inputs\n",
    "        differences = fake_data - real_data\n",
    "        interpolates = real_data + \\\n",
    "        tf.transpose(alpha*tf.transpose(differences, perm=[1,2,3,0]), [3,0,1,2])\n",
    "        _, d_inter = self.discriminator(interpolates, reuse=True) \n",
    "        gradients = tf.gradients(d_inter, [interpolates])[0]\n",
    "        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))\n",
    "        self.gradient_penalty = tf.reduce_mean((slopes-1.)**2)\n",
    "      else:\n",
    "        print(\"ERROR: Unrecognized f-divergence...exiting\")\n",
    "        exit(-1)\n",
    "\n",
    "    self.d_loss_real_sum = scalar_summary(\"d_loss_real\", self.d_loss_real)\n",
    "    self.d_loss_fake_sum = scalar_summary(\"d_loss_fake\", self.d_loss_fake)\n",
    "                          \n",
    "    if self.f_div == \"wgan\":\n",
    "      self.d_loss = self.d_loss_real + self.d_loss_fake + self.reg * self.gradient_penalty\n",
    "    else:\n",
    "      self.d_loss = self.d_loss_real + self.d_loss_fake\n",
    "\n",
    "    self.g_loss_sum = scalar_summary(\"g_loss\", self.g_loss)\n",
    "    self.d_loss_sum = scalar_summary(\"d_loss\", self.d_loss)\n",
    "\n",
    "    t_vars = tf.trainable_variables()\n",
    "\n",
    "    self.d_vars = [var for var in t_vars if '/d_' in var.name]\n",
    "    self.g_vars = [var for var in t_vars if '/g_' in var.name]\n",
    "    print(\"gen_vars:\")\n",
    "    for var in self.g_vars:\n",
    "      print(var.name)\n",
    "\n",
    "    print(\"disc_vars:\")\n",
    "    for var in self.d_vars:\n",
    "      print(var.name)\n",
    "    \n",
    "    self.saver = tf.train.Saver(max_to_keep=0)\n",
    "\n",
    "  def evaluate_neg_loglikelihood(self, data, config):\n",
    "    log_like_batch_idxs = len(data) // config.batch_size\n",
    "    lli_list = []\n",
    "    inter_list = []\n",
    "    for idx in xrange(0, log_like_batch_idxs):\n",
    "      batch_images = data[idx*config.batch_size:(idx+1)*config.batch_size]\n",
    "      batch_images = np.cast[np.float32](batch_images)\n",
    "      \n",
    "      if self.model_type == \"nice\":\n",
    "        batch_images = batch_images[:,self.permutation]\n",
    "\n",
    "      lli = self.sess.run([self.log_likelihood],\n",
    "        feed_dict={self.log_like_batch: batch_images})\n",
    "      \n",
    "      lli_list.append(lli)\n",
    "\n",
    "    return np.mean(lli_list)\n",
    "\n",
    "  def train(self, config, train_data=None, val_data=None):\n",
    "    data_X = train_data \n",
    "    seed = 0\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "    \"\"\"Train DCGAN\"\"\"\n",
    "    if config.dataset == \"mnist\":\n",
    "      data_X, val_data, test_data, train_dist = mnist_data.load_mnist()\n",
    "    elif config.dataset == \"cifar\":\n",
    "      data_X, val_data, test_data = cifar_data.load_cifar()\n",
    "\n",
    "    if self.model_type == \"nice\":\n",
    "      val_data = np.reshape(val_data, (-1,self.image_size))\n",
    "      test_data = np.reshape(test_data, (-1, self.image_size))\n",
    "\n",
    "    lr = config.learning_rate\n",
    "    self.learning_rate = tf.placeholder(tf.float32, [], name='lr')\n",
    "\n",
    "    d_optim_ = tf.train.AdamOptimizer(self.learning_rate, beta1=config.beta1, beta2=0.9)\n",
    "    d_grad = d_optim_.compute_gradients(self.d_loss, var_list=self.d_vars)\n",
    "    d_grad_mag = tf.global_norm(d_grad)\n",
    "    d_optim = d_optim_.apply_gradients(d_grad)          \n",
    "\n",
    "    g_optim_ = tf.train.AdamOptimizer(self.learning_rate, beta1=config.beta1, beta2=0.9)\n",
    "    if self.n_critic <= 0:\n",
    "      g_grad = g_optim_.compute_gradients(self.train_log_likelihood\\\n",
    "          , var_list=self.g_vars)\n",
    "    else:\n",
    "      if self.like_reg > 0:\n",
    "        if self.model_type == \"real_nvp\":\n",
    "          g_grad_1 = g_optim_.compute_gradients(self.g_loss / self.like_reg, var_list=self.g_vars)\n",
    "          g_grad_2 = g_optim_.compute_gradients(self.train_log_likelihood, var_list=self.g_vars)\n",
    "          grads_1, _ = zip(*g_grad_1)\n",
    "          grads_2, _ = zip(*g_grad_2)\n",
    "          sum_grad = [g1+g2 for g1, g2 in zip(grads_1, grads_2)]\n",
    "          g_grad = [pair for pair in zip(sum_grad, [var for grad, var in g_grad_1])]\n",
    "        else:\n",
    "          g_grad = g_optim_.compute_gradients(self.g_loss/self.like_reg + self.train_log_likelihood ,var_list=self.g_vars)  \n",
    "      else:\n",
    "        g_grad = g_optim_.compute_gradients(self.g_loss, var_list=self.g_vars)\n",
    "\n",
    "    \n",
    "    g_grad_mag = tf.global_norm(g_grad)\n",
    "    g_optim = g_optim_.apply_gradients(g_grad)         \n",
    "\n",
    "    try: ##for data-dependent init (not implemented)\n",
    "      if self.model_type == \"real_nvp\":\n",
    "        self.sess.run(tf.global_variables_initializer(),\n",
    "          {self.x_init: data_X[0:config.batch_size]})\n",
    "      else:\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    except:\n",
    "      if self.model_type == \"real_nvp\":\n",
    "        self.sess.run(tf.global_variables_initializer(),\n",
    "          {self.x_init: data_X[0:config.batch_size]})\n",
    "      else:\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    self.g_sum = merge_summary([self.z_sum, self.d__sum,\n",
    "      self.G_sum, self.d_loss_fake_sum, self.g_loss_sum])\n",
    "    self.d_sum = merge_summary(\n",
    "        [self.z_sum, self.d_sum, self.d_loss_real_sum, self.d_loss_sum])\n",
    "    log_dir = \"./\" + self.log_dir\n",
    "    if os.path.exists(log_dir):\n",
    "        shutil.rmtree(log_dir)\n",
    "    os.makedirs(log_dir)\n",
    "    # self.writer = SummaryWriter(log_dir)\n",
    "\n",
    "    counter = 1\n",
    "    start_time = time.time()\n",
    "    could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
    "    if could_load:\n",
    "      counter = checkpoint_counter\n",
    "      print(\" [*] Load SUCCESS\")\n",
    "    else:\n",
    "      print(\" [!] Load failed...\")\n",
    "\n",
    "    ############## A FIXED BATCH OF Zs FOR GENERATING SAMPLES ######################\n",
    "    if self.prior == \"uniform\":\n",
    "      sample_z = np.random.uniform(-1, 1, size=(self.sample_num , self.z_dim))\n",
    "    elif self.prior == \"logistic\":\n",
    "      sample_z = np.random.logistic(loc=0., scale=1., size=(self.sample_num , self.z_dim))\n",
    "    elif self.prior == \"gaussian\":\n",
    "      sample_z = np.random.normal(0.0, 1.0, size=(self.sample_num , self.z_dim))\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized prior...exiting\")\n",
    "        exit(-1)\n",
    "\n",
    "    ################################ Evaluate initial model lli ########################\n",
    "\n",
    "    val_nlli = self.evaluate_neg_loglikelihood(val_data, config)\n",
    "    # train_nlli = self.evaluate_neg_loglikelihood(train_data, config)\n",
    "\n",
    "    # curr_inception_score = self.calculate_inception_and_mode_score()\n",
    "    # print(\"INITIAL TEST: val neg logli: %.8f,incep score: %.8f\" % (val_nlli,\\\n",
    "    #  curr_inception_score[0]))\n",
    "    if counter > 1:\n",
    "      old_data = np.load(\"./\"+config.sample_dir+'/graph_data.npy') \n",
    "      self.best_val_nlli = old_data[2]\n",
    "      self.best_model_counter = old_data[3]\n",
    "      self.best_model_path = old_data[4]\n",
    "      self.val_nlli_list = old_data[1]\n",
    "      self.counter_list = old_data[5]\n",
    "      self.batch_train_nlli_list = old_data[-4]\n",
    "      self.inception_list = old_data[-2]\n",
    "      self.samples_list = old_data[0]\n",
    "      self.loss_list = old_data[-1]\n",
    "      manifold_h, manifold_w = old_data[6]\n",
    "    # else:\n",
    "    #   try:\n",
    "    #     self.writer.add_summary(tf.Summary(\\\n",
    "    #           value=[tf.Summary.Value(tag=\"Val Neg Log-likelihood\", simple_value=val_nlli)]), counter)\n",
    "    #   except Exception as e:\n",
    "    #     print(\"[!] Failed to write summary:\", e)  \n",
    "      # self.writer.add_summary(tf.Summary(\\\n",
    "      #         value=[tf.Summary.Value(tag=\"Train Neg Log-likelihood\", simple_value=train_nlli)]), counter)\n",
    "\n",
    "      self.best_val_nlli = val_nlli\n",
    "      # self.best_model_train_nlli = train_nlli\n",
    "      self.best_model_counter = counter\n",
    "      self.best_model_path = self.save(config.checkpoint_dir, counter)\n",
    "      # self.train_nlli_list = [train_nlli]\n",
    "      self.val_nlli_list = [val_nlli]\n",
    "      self.counter_list = [1]\n",
    "      self.batch_train_nlli_list = []\n",
    "      self.inception_list = [None]\n",
    "      self.samples_list = self.sess.run([self.sampler],\n",
    "              feed_dict={\n",
    "                  self.z: sample_z,\n",
    "              }\n",
    "            )\n",
    "      sample_inputs = data_X[0:config.batch_size]\n",
    "      samples = self.samples_list[0]\n",
    "      manifold_h = int(np.ceil(np.sqrt(samples.shape[0])))\n",
    "      manifold_w = int(np.floor(np.sqrt(samples.shape[0])))\n",
    "      self.loss_list = self.sess.run(\n",
    "              [self.d_loss_real, self.d_loss_fake],\n",
    "              feed_dict={\n",
    "                  self.z: sample_z,\n",
    "                  self.inputs: sample_inputs,\n",
    "              })\n",
    "    ##################################################################################\n",
    "\n",
    "    for epoch in xrange(config.epoch):\n",
    "      np.random.shuffle(data_X)\n",
    "      batch_idxs = len(data_X) // config.batch_size\n",
    "      \n",
    "      for idx in xrange(0, batch_idxs):\n",
    "        sys.stdout.flush()\n",
    "        batch_images = data_X[idx*config.batch_size:(idx+1)*config.batch_size]\n",
    "        \n",
    "        if self.prior == \"uniform\":\n",
    "          batch_z = np.random.uniform(-1, 1, [config.batch_size, self.z_dim]) \\\n",
    "              .astype(np.float32)\n",
    "        elif self.prior == \"logistic\":\n",
    "          batch_z = np.random.logistic(loc=0.,scale=1.0,size=[config.batch_size, self.z_dim]) \\\n",
    "              .astype(np.float32)\n",
    "        elif self.prior == \"gaussian\":\n",
    "          batch_z = np.random.normal(0.0, 1.0, size=(config.batch_size , self.z_dim))\n",
    "        else:\n",
    "          print(\"ERROR: Unrecognized prior...exiting\")\n",
    "          exit(-1)\n",
    "\n",
    "        for r in range(self.n_critic):\n",
    "          _, d_g_mag, errD_fake, errD_real ,summary_str = self.sess.run([d_optim, d_grad_mag, \n",
    "            self.d_loss_fake, self.d_loss_real, self.d_sum],\n",
    "            feed_dict={ \n",
    "              self.inputs: batch_images,\n",
    "              self.z: batch_z,\n",
    "              self.learning_rate:lr,\n",
    "            })\n",
    "        # if self.n_critic > 0:\n",
    "        #   self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "        # Update G network\n",
    "        if self.like_reg > 0 or self.n_critic <= 0:\n",
    "          _, g_g_mag, errG, summary_str = self.sess.run([g_optim, g_grad_mag, self.g_loss, self.g_sum],\n",
    "            feed_dict={\n",
    "              self.z: batch_z, \n",
    "              self.learning_rate:lr,\n",
    "              self.inputs: batch_images,\n",
    "            })\n",
    "        else:\n",
    "          _, g_g_mag ,errG, summary_str = self.sess.run([g_optim, g_grad_mag, self.g_loss, self.g_sum],\n",
    "            feed_dict={\n",
    "              self.z: batch_z, \n",
    "              self.learning_rate:lr,\n",
    "            })\n",
    "        # self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "        batch_images_nl = batch_images\n",
    "        if self.model_type == \"nice\":\n",
    "          batch_images_nl = np.reshape(batch_images_nl,(self.batch_size, -1))[:,self.permutation]\n",
    "        b_train_nlli = self.sess.run([self.log_likelihood], feed_dict={\n",
    "          self.log_like_batch: batch_images_nl,\n",
    "          })\n",
    "        b_train_nlli = b_train_nlli[0]\n",
    "\n",
    "        self.batch_train_nlli_list.append(b_train_nlli)\n",
    "        # if self.n_critic > 0:\n",
    "        #   self.loss_list.append([errD_real, errD_fake])\n",
    "        #   self.writer.add_summary(tf.Summary(\\\n",
    "        #   value=[tf.Summary.Value(tag=\"training loss\", simple_value=-(errD_fake+errD_real))]) ,counter)\n",
    "        # self.writer.add_summary(tf.Summary(\\\n",
    "        #   value=[tf.Summary.Value(tag=\"Batch train Neg Log-likelihood\", simple_value=b_train_nlli)]) ,counter)\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "        lr = max(lr * self.lr_decay, self.min_lr)\n",
    "\n",
    "        if np.mod(counter, 703) == 1: #340\n",
    "          if self.n_critic > 0:\n",
    "            print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f, d_grad_mag: %.8f, g_grad_mag: %.8f, lr: %.8f\" \\\n",
    "          % (epoch, idx, batch_idxs,\n",
    "            time.time() - start_time, errD_fake+errD_real, errG, d_g_mag, g_g_mag, lr))\n",
    "          else:\n",
    "            print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, g_loss: %.8f, g_grad_mag: %.8f, lr: %.8f\" \\\n",
    "          % (epoch, idx, batch_idxs,\n",
    "            time.time() - start_time, errG, g_g_mag, lr))\n",
    "          curr_model_path = self.save(config.checkpoint_dir, counter)\n",
    "\n",
    "          val_nlli=self.evaluate_neg_loglikelihood(val_data, config)\n",
    "\n",
    "          # train_nlli = self.evaluate_neg_loglikelihood(train_data, config)\n",
    "          curr_inception_score = self.calculate_inception_and_mode_score()\n",
    "\n",
    "          print(\"[LogLi (%d,%d)]: val neg logli: %.8f, ince: %.8f, train lli: %.8f\" % (epoch, idx,val_nlli,\\\n",
    "           curr_inception_score[0], np.mean(self.batch_train_nlli_list[-700:])))\n",
    "\n",
    "          # self.writer.add_summary(tf.Summary(\\\n",
    "          #         value=[tf.Summary.Value(tag=\"Val Neg Log-likelihood\", simple_value=val_nlli)]), counter)\n",
    "          # self.writer.add_summary(tf.Summary(\\\n",
    "          #         value=[tf.Summary.Value(tag=\"Train Neg Log-likelihood\", simple_value=train_nlli)]), counter)\n",
    "          if val_nlli < self.best_val_nlli:\n",
    "            self.best_val_nlli = val_nlli\n",
    "            self.best_model_counter = counter\n",
    "            self.best_model_path = curr_model_path\n",
    "            # self.best_model_train_nlli = train_nlli\n",
    "          # self.train_nlli_list.append(train_nlli)\n",
    "          self.val_nlli_list.append(val_nlli)\n",
    "          self.counter_list.append(counter)\n",
    "\n",
    "          samples, d_loss, g_loss = self.sess.run(\n",
    "            [self.sampler, self.d_loss, self.g_loss],\n",
    "            feed_dict={\n",
    "                self.z: sample_z,\n",
    "                self.inputs: sample_inputs,\n",
    "            }\n",
    "          )\n",
    "          self.samples_list.append(samples)\n",
    "          self.samples_list[-1].shape[1]\n",
    "          manifold_h = int(np.ceil(np.sqrt(samples.shape[0])))\n",
    "          manifold_w = int(np.floor(np.sqrt(samples.shape[0])))\n",
    "          self.inception_list.append(curr_inception_score)\n",
    "          save_images(samples, [manifold_h, manifold_w],\n",
    "                './{}/train_{:02d}_{:04d}.png'.format(config.sample_dir, epoch, idx))\n",
    "          print(\"[Sample] d_loss: %.8f, g_loss: %.8f\" % (d_loss, g_loss))\n",
    "\n",
    "          np.save(\"./\"+config.sample_dir+'/graph_data', \n",
    "            [self.samples_list, self.val_nlli_list, self.best_val_nlli, self.best_model_counter,\\\n",
    "             self.best_model_path, self.counter_list, [manifold_h, manifold_w], \\\n",
    "             self.batch_train_nlli_list, self.inception_list, self.loss_list])\n",
    "\n",
    "    \n",
    "    np.save(\"./\"+config.sample_dir+'/graph_data', \n",
    "            [self.samples_list, self.val_nlli_list, self.best_val_nlli, self.best_model_counter,\\\n",
    "             self.best_model_path, self.counter_list, [manifold_h, manifold_w], \\\n",
    "             self.batch_train_nlli_list, self.inception_list, self.loss_list])\n",
    "    self.test_model(test_data, config)\n",
    "\n",
    "  def test_model(self, test_data, config):\n",
    "    print(\"[*] Restoring best model counter: %d, val neg lli: %.8f\" \n",
    "      % (self.best_model_counter, self.best_val_nlli))\n",
    "    self.saver.restore(self.sess, self.best_model_path)\n",
    "    print(\"[*] Best model restore from: \" + self.best_model_path)\n",
    "    print(\"[*] Evaluating on the test set\")\n",
    "    test_nlli = self.evaluate_neg_loglikelihood(test_data, config)\n",
    "    print(\"[*] Test negative log likelihood: %.8f\" % (test_nlli))\n",
    "\n",
    "  def calculate_inception_and_mode_score(self):\n",
    "    #to get mode scores add code to load your favourite mnist classifier in inception_score.py\n",
    "    if self.dataset_name == \"mnist\": \n",
    "      return [0.0, 0.0, 0.0, 0.0]\n",
    "    sess = self.sess\n",
    "    all_samples = []\n",
    "    for i in range(18):\n",
    "        if self.prior == \"uniform\":\n",
    "          batch_z = np.random.uniform(-1, 1, [self.batch_size, self.z_dim]) \\\n",
    "              .astype(np.float32)\n",
    "        elif self.prior == \"logistic\":\n",
    "          batch_z = np.random.logistic(loc=0.,scale=1.0,size=[self.batch_size, self.z_dim]) \\\n",
    "              .astype(np.float32)\n",
    "        elif self.prior == \"gaussian\":\n",
    "          batch_z = np.random.normal(0.0, 1.0, size=(self.batch_size , self.z_dim))\n",
    "        else:\n",
    "          print(\"ERROR: Unrecognized prior...exiting\")\n",
    "          exit(-1)\n",
    "        samples_curr = self.sess.run(\n",
    "            [self.sampler],\n",
    "            feed_dict={\n",
    "                self.z: batch_z,}\n",
    "          )\n",
    "        all_samples.append(samples_curr[0])\n",
    "    all_samples = np.concatenate(all_samples, axis=0)\n",
    "    # return all_samples\n",
    "    all_samples = (all_samples*255.).astype('int32')\n",
    "    \n",
    "    return inception_score.get_inception_and_mode_score(list(all_samples), sess=sess)\n",
    "  \n",
    "  def discriminator(self, image, y=None, reuse=False):\n",
    "    with tf.variable_scope(\"discriminator\") as scope:\n",
    "      tf.set_random_seed(0)\n",
    "      np.random.seed(0)\n",
    "      if reuse:\n",
    "        scope.reuse_variables()\n",
    "\n",
    "      if self.dataset_name != \"mnist\":\n",
    "        if self.f_div == \"wgan\":\n",
    "          hn1 = image\n",
    "         \n",
    "          h0 = Layernorm('d_ln_1', [1,2,3], lrelu(conv2d(hn1, self.df_dim , name='d_h0_conv')))\n",
    "          h1 = Layernorm('d_ln_2', [1,2,3], lrelu(conv2d(h0, self.df_dim*2, name='d_h1_conv')))\n",
    "          h2 = Layernorm('d_ln_3', [1,2,3], lrelu(conv2d(h1, self.df_dim*4, name='d_h2_conv')))\n",
    "          h3 = Layernorm('d_ln_4', [1,2,3], lrelu(conv2d(h2, self.df_dim*8, name='d_h3_conv')))\n",
    "          h4 = linear(tf.reshape(h3, [self.batch_size, -1]), 1, 'd_h3_lin')\n",
    "      \n",
    "          return tf.sigmoid(h4), h4\n",
    "        else:\n",
    "          h0 = lrelu(conv2d(image, self.df_dim, name='d_h0_conv'))\n",
    "          h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name='d_h1_conv')))\n",
    "          h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*4, name='d_h2_conv')))\n",
    "          h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*8, name='d_h3_conv')))\n",
    "          h4 = linear(tf.reshape(h3, [self.batch_size, -1]), 1, 'd_h3_lin')\n",
    "\n",
    "          return tf.sigmoid(h4), h4\n",
    "      else:\n",
    "        if self.f_div == \"wgan\":\n",
    "          x = image\n",
    "\n",
    "          h0 = lrelu(conv2d(x, self.c_dim, name='d_h0_conv'))\n",
    "\n",
    "          h1 = lrelu(conv2d(h0, self.df_dim , name='d_h1_conv'))\n",
    "          h1 = tf.reshape(h1, [self.batch_size, -1])      \n",
    "\n",
    "          h2 = lrelu(linear(h1, self.dfc_dim, 'd_h2_lin'))\n",
    "\n",
    "          h3 = linear(h2, 1, 'd_h3_lin')\n",
    "\n",
    "          return tf.sigmoid(h3), h3\n",
    "        else:\n",
    "          x = image\n",
    "          \n",
    "          h0 = lrelu(conv2d(x, self.c_dim, name='d_h0_conv'))\n",
    "          \n",
    "          h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim , name='d_h1_conv')))\n",
    "          h1 = tf.reshape(h1, [self.batch_size, -1])      \n",
    "          \n",
    "          h2 = lrelu(self.d_bn2(linear(h1, self.dfc_dim, 'd_h2_lin')))\n",
    "          \n",
    "          h3 = linear(h2, 1, 'd_h3_lin')\n",
    "            \n",
    "          return tf.sigmoid(h3), h3\n",
    "\n",
    "\n",
    "  @property\n",
    "  def model_dir(self):\n",
    "    return \"{}_{}_{}_{}\".format(\n",
    "        self.dataset_name, self.batch_size,\n",
    "        self.input_height, self.input_width)\n",
    "      \n",
    "  def save(self, checkpoint_dir, step):\n",
    "    model_name = \"DCGAN.model\"\n",
    "    checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "      os.makedirs(checkpoint_dir)\n",
    "\n",
    "    return self.saver.save(self.sess,\n",
    "            os.path.join(checkpoint_dir, model_name),\n",
    "            global_step=step)\n",
    "\n",
    "  def load(self, checkpoint_dir):\n",
    "    import re\n",
    "    print(\" [*] Reading checkpoints...\")\n",
    "    checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "      ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "      self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "      counter = int(next(re.finditer(r\"(\\d+)(?!.*\\d)\", ckpt_name)).group(0))\n",
    "      print(\" [*] Success to read {}\".format(ckpt_name))\n",
    "      return True, counter\n",
    "    else:\n",
    "      print(\" [*] Failed to find a checkpoint\")\n",
    "      return False, 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class batch_norm(object):\n",
    "    def __init__(self, epsilon=1e-5, momentum=0.9, name=\"batch_norm\"):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.name = name\n",
    "        self.bn_layer = tf.keras.layers.BatchNormalization(\n",
    "            momentum=self.momentum,\n",
    "            epsilon=self.epsilon,\n",
    "            name=self.name\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        return self.bn_layer(x, training=train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "  image_summary = tf.image_summary\n",
    "  scalar_summary = tf.scalar_summary\n",
    "  histogram_summary = tf.histogram_summary\n",
    "  merge_summary = tf.merge_summary\n",
    "  SummaryWriter = tf.train.SummaryWriter\n",
    "  tf.set_random_seed(0)\n",
    "except:\n",
    "  image_summary = tf.summary.image\n",
    "  scalar_summary = tf.summary.scalar\n",
    "  histogram_summary = tf.summary.histogram\n",
    "  merge_summary = tf.summary.merge\n",
    "  SummaryWriter = tf.summary.FileWriter\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "# Load reshaped data\n",
    "data_X = np.load(FLAGS.input_file)\n",
    "data_X = data_X.reshape([-1, FLAGS.input_height, FLAGS.input_width, FLAGS.c_dim])\n",
    "train_data = data_X[:-500]\n",
    "val_data = data_X[-500:]\n",
    "\n",
    "dcgan = DCGAN(\n",
    "    sess,\n",
    "    input_width=FLAGS.input_width,\n",
    "    input_height=FLAGS.input_height,\n",
    "    batch_size=FLAGS.batch_size,\n",
    "    sample_num=FLAGS.batch_size,\n",
    "    c_dim=FLAGS.c_dim,\n",
    "    z_dim=FLAGS.input_height * FLAGS.input_width * FLAGS.c_dim,\n",
    "    dataset_name=FLAGS.dataset,\n",
    "    checkpoint_dir=FLAGS.checkpoint_dir,\n",
    "    f_div=FLAGS.f_div,\n",
    "    prior=FLAGS.prior,\n",
    "    lr_decay=FLAGS.lr_decay,\n",
    "    min_lr=FLAGS.min_lr,\n",
    "    model_type=FLAGS.model_type,\n",
    "    log_dir=FLAGS.flow_log_dir,\n",
    "    alpha=FLAGS.alpha,\n",
    "    batch_norm_adaptive=FLAGS.batch_norm_adaptive,\n",
    "    init_type=FLAGS.init_type,\n",
    "    reg=FLAGS.reg,\n",
    "    n_critic=FLAGS.n_critic,\n",
    "    hidden_layers=FLAGS.hidden_layers,\n",
    "    no_of_layers=FLAGS.no_of_layers,\n",
    "    like_reg=FLAGS.like_reg,\n",
    "    df_dim=FLAGS.df_dim,\n",
    "    flags=FLAGS\n",
    ")\n",
    "\n",
    "dcgan.train(FLAGS, train_data=train_data, val_data=val_data)\n",
    "\n",
    "\n",
    "z_sample = np.random.normal(0.0, 1.0, size=(FLAGS.batch_size, FLAGS.input_height * FLAGS.input_width * FLAGS.c_dim))\n",
    "synth_returns = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample})\n",
    "synth_returns = synth_returns.squeeze()\n",
    "print(\"Synthetic Returns Shape:\", synth_returns.shape)\n",
    "\n",
    "\n",
    "from postprocess import reconstruct_prices\n",
    "\n",
    "last_real_price = df.iloc[window_size - 1].values\n",
    "synthetic_prices = reconstruct_prices(synth_returns, last_real_price)\n",
    "\n",
    "# Plot samples\n",
    "for i in range(5):\n",
    "    plt.plot(synthetic_prices[i])\n",
    "plt.title(\"Synthetic Price Paths (sample)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
