{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dd0e6e-761e-4f91-b887-9bdd100db492",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "g_h4_out: (4, 4)\n",
      "g_h3_out: (2, 2)\n",
      "g_h2_out: (1, 1)\n",
      "g_h1_out: (1, 1)\n",
      "g_h0_out: (1, 1)\n",
      "****\n",
      "g_h4_W: (5, 5, 1, 96)\n",
      "g_h4_b: (1,)\n",
      "g_h3_W: (3, 3, 96, 192)\n",
      "g_h3_b: (96,)\n",
      "g_h2_W: (3, 3, 192, 384)\n",
      "g_h2_b: (192,)\n",
      "g_h1_W: (3, 3, 384, 512)\n",
      "g_h1_b: (384,)\n",
      "g_h0_lin_W: (100, 512)\n",
      "g_h0_lin_b: (512,)\n",
      "****\n",
      "d_h0_W: (5, 5, 1, 96)\n",
      "d_h0_b: (96,)\n",
      "d_h1_W: (3, 3, 96, 192)\n",
      "d_h1_b: (192,)\n",
      "d_h2_W: (3, 3, 192, 384)\n",
      "d_h2_b: (384,)\n",
      "d_h3_W: (3, 3, 384, 512)\n",
      "d_h3_b: (512,)\n",
      "d_h_end_lin_W: (512, 512)\n",
      "d_h_end_lin_b: (512,)\n",
      "d_h_out_lin_W: (512, 2)\n",
      "d_h_out_lin_b: (2,)\n",
      "WARNING:tensorflow:From /home/rootadmin/miniconda3/envs/bgan_env36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From <ipython-input-1-f13f123d266f>:368: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /home/rootadmin/miniconda3/envs/bgan_env36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py:2509: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Starting session\n",
      "Starting training loop\n"
     ]
    }
   ],
   "source": [
    "# Standard\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Monkey patch tf.ones_like to avoid ValueError in tfp.internal.prefer_static\n",
    "def patched_ones_like(input, dtype=None, name=None):\n",
    "    return tf.ones_like(input, dtype=dtype, name=name)\n",
    "tf.ones_like = patched_ones_like\n",
    "\n",
    "from imageio import imread\n",
    "import scipy.io as sio\n",
    "\n",
    "import math\n",
    "from math import ceil\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "df = pd.read_csv(\"raw (FX + EQ).csv\")\n",
    "data = df.values\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Optionally: work with returns or PCA-reduced data\n",
    "# returns = np.diff(np.log(data + 1e-5), axis=0)\n",
    "\n",
    "class FXEQDataset:\n",
    "    def __init__(self, data, batch_size):\n",
    "        self.original_dim = data.shape[1]  # e.g., 12\n",
    "        padded_dim = 16  # make it square (4x4)\n",
    "        if self.original_dim < padded_dim:\n",
    "            # pad with zeros\n",
    "            data = np.pad(data, ((0, 0), (0, padded_dim - self.original_dim)), mode='constant')\n",
    "        elif self.original_dim > padded_dim:\n",
    "            raise ValueError(\"Too many features to fit into 4x4 image.\")\n",
    "\n",
    "        self.data = data.reshape(-1, 4, 4, 1).astype(np.float32)  # reshape to image\n",
    "        self.batch_size = batch_size\n",
    "        self.idx = 0\n",
    "        self.num_samples = self.data.shape[0]\n",
    "        self.x_dim = [4, 4, 1]\n",
    "        self.dataset_size = self.num_samples\n",
    "\n",
    "    def next_batch(self):\n",
    "        if self.idx + self.batch_size > self.num_samples:\n",
    "            self.idx = 0\n",
    "        batch = self.data[self.idx:self.idx + self.batch_size]\n",
    "        self.idx += self.batch_size\n",
    "        return batch\n",
    "\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    return tf.maximum(x, leak * x, name=name)\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    error = y_true - y_pred\n",
    "    abs_error = tf.abs(error)\n",
    "    quadratic = tf.minimum(abs_error, delta)\n",
    "    linear = abs_error - quadratic\n",
    "    return 0.5 * tf.square(quadratic) + delta * linear\n",
    "\n",
    "tf.reset_default_graph()  # âœ… Reset before anything is built\n",
    "\n",
    "# Set up BDCGAN\n",
    "\n",
    "def conv_out_size(size, stride):\n",
    "    co = int(math.ceil(size / float(stride)))\n",
    "    return co\n",
    "\n",
    "def kernel_sizer(size, stride):\n",
    "    ko = int(math.ceil(size / float(stride)))\n",
    "    if ko % 2 == 0:\n",
    "        ko += 1\n",
    "    return ko\n",
    "\n",
    "def conv2d(input_, output_dim, \n",
    "           k_h=5, k_w=5, d_h=2, d_w=2, \n",
    "           stddev=0.02, name=\"conv2d\", \n",
    "           w=None, biases=None, padding=\"SAME\"):\n",
    "    with tf.variable_scope(name):\n",
    "        if w is None:\n",
    "            w = tf.get_variable(\"w\", [k_h, k_w, input_.get_shape()[-1], output_dim],\n",
    "                                initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding=padding)\n",
    "        if biases is None:\n",
    "            biases = tf.get_variable(\"biases\", [output_dim], initializer=tf.constant_initializer(0.0))\n",
    "        conv = tf.nn.bias_add(conv, biases)\n",
    "        return conv\n",
    "\n",
    "def deconv2d(input_, output_shape,\n",
    "             k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n",
    "             name=\"deconv2d\", w=None, biases=None, padding=\"SAME\"):\n",
    "    with tf.variable_scope(name):\n",
    "        if w is None:\n",
    "            w = tf.get_variable(\"w\", [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],\n",
    "                                initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "        deconv = tf.nn.conv2d_transpose(input_, w,\n",
    "                                        output_shape=output_shape,\n",
    "                                        strides=[1, d_h, d_w, 1],\n",
    "                                        padding=padding)\n",
    "        if biases is None:\n",
    "            biases = tf.get_variable(\"biases\", [output_shape[-1]],\n",
    "                                     initializer=tf.constant_initializer(0.0))\n",
    "        deconv = tf.nn.bias_add(deconv, biases)\n",
    "        return deconv\n",
    "\n",
    "\n",
    "\n",
    "class BDCGAN(object):\n",
    "\n",
    "    def __init__(self, x_dim, z_dim, dataset_size, batch_size=64, gf_dim=64, df_dim=64, \n",
    "                 prior_std=1.0, J=1, M=1, eta=2e-4, num_layers=4,\n",
    "                 alpha=0.01, lr=0.0002, optimizer='adam', wasserstein=False, \n",
    "                 ml=False, J_d=None):\n",
    "\n",
    "        if len(x_dim) == 1:\n",
    "            c_dim = 1\n",
    "            s_h = s_w = int(np.ceil(np.sqrt(x_dim[0])))  # reshape to square-like image\n",
    "            self.x_dim = [s_h, s_w, 1]\n",
    "        else:\n",
    "            c_dim = x_dim[2]\n",
    "            self.x_dim = x_dim\n",
    "        self.is_grayscale = (c_dim == 1)\n",
    "        self.optimizer = optimizer.lower()\n",
    "        self.dataset_size = dataset_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.K = 2 # fake and real classes\n",
    "        # self.x_dim = x_dim\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.gf_dim = gf_dim\n",
    "        self.df_dim = df_dim\n",
    "        self.c_dim = c_dim\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Bayes\n",
    "        self.prior_std = prior_std\n",
    "        self.num_gen = J\n",
    "        self.num_disc = J_d if J_d is not None else 1\n",
    "        self.num_mcmc = M\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        # ML\n",
    "        self.ml = ml\n",
    "        if self.ml:\n",
    "            assert self.num_gen == 1 and self.num_disc == 1 and self.num_mcmc == 1, \"invalid settings for ML training\"\n",
    "\n",
    "        self.noise_std = np.sqrt(2 * self.alpha * self.eta)\n",
    "\n",
    "        def get_strides(num_layers, num_pool):\n",
    "            interval = int(math.floor(num_layers/float(num_pool)))\n",
    "            strides = np.array([1]*num_layers)\n",
    "            strides[0:interval*num_pool:interval] = 2\n",
    "            return strides\n",
    "\n",
    "        self.num_pool = 4\n",
    "        self.max_num_dfs = 512\n",
    "        self.gen_strides = get_strides(num_layers, self.num_pool)\n",
    "        self.disc_strides = self.gen_strides\n",
    "        num_dfs = np.cumprod(np.array([self.df_dim] + list(self.disc_strides)))[:-1]\n",
    "        num_dfs[num_dfs >= self.max_num_dfs] = self.max_num_dfs # memory\n",
    "        self.num_dfs = list(num_dfs)\n",
    "        self.num_gfs = self.num_dfs[::-1]\n",
    "\n",
    "        self.construct_from_hypers(gen_strides=self.gen_strides, disc_strides=self.disc_strides,\n",
    "                                   num_gfs=self.num_gfs, num_dfs=self.num_dfs)\n",
    "        \n",
    "        self.build_bgan_graph()\n",
    "\n",
    "\n",
    "    def construct_from_hypers(self, gen_kernel_size=5, gen_strides=[2,2,2,2],\n",
    "                              disc_kernel_size=5, disc_strides=[2,2,2,2],\n",
    "                              num_dfs=None, num_gfs=None):\n",
    "\n",
    "        \n",
    "        self.d_batch_norm = AttributeDict([(\"d_bn%i\" % dbn_i, batch_norm(name='d_bn%i' % dbn_i)) for dbn_i in range(len(disc_strides))])\n",
    "        self.sup_d_batch_norm = AttributeDict([(\"sd_bn%i\" % dbn_i, batch_norm(name='sup_d_bn%i' % dbn_i)) for dbn_i in range(5)])\n",
    "        self.g_batch_norm = AttributeDict([(\"g_bn%i\" % gbn_i, batch_norm(name='g_bn%i' % gbn_i)) for gbn_i in range(len(gen_strides))])\n",
    "\n",
    "        if num_dfs is None:\n",
    "            num_dfs = [self.df_dim, self.df_dim*2, self.df_dim*4, self.df_dim*8]\n",
    "            \n",
    "        if num_gfs is None:\n",
    "            num_gfs = [self.gf_dim*8, self.gf_dim*4, self.gf_dim*2, self.gf_dim]\n",
    "\n",
    "        assert len(gen_strides) == len(num_gfs), \"invalid hypers!\"\n",
    "        assert len(disc_strides) == len(num_dfs), \"invalid hypers!\"\n",
    "\n",
    "        s_h, s_w = self.x_dim[0], self.x_dim[1]\n",
    "        ks = gen_kernel_size\n",
    "        self.gen_output_dims = OrderedDict()\n",
    "        self.gen_weight_dims = OrderedDict()\n",
    "        num_gfs = num_gfs + [self.c_dim]\n",
    "        self.gen_kernel_sizes = [ks]\n",
    "        for layer in range(len(gen_strides))[::-1]:\n",
    "            self.gen_output_dims[\"g_h%i_out\" % (layer+1)] = (s_h, s_w)\n",
    "            assert gen_strides[layer] <= 2, \"invalid stride\"\n",
    "            assert ks % 2 == 1, \"invalid kernel size\"\n",
    "            self.gen_weight_dims[\"g_h%i_W\" % (layer+1)] = (ks, ks, num_gfs[layer+1], num_gfs[layer])\n",
    "            self.gen_weight_dims[\"g_h%i_b\" % (layer+1)] = (num_gfs[layer+1],)\n",
    "            s_h, s_w = conv_out_size(s_h, gen_strides[layer]), conv_out_size(s_w, gen_strides[layer])\n",
    "            ks = kernel_sizer(ks, gen_strides[layer])\n",
    "            self.gen_kernel_sizes.append(ks)\n",
    "\n",
    "\n",
    "        self.gen_weight_dims.update(OrderedDict([(\"g_h0_lin_W\", (self.z_dim, num_gfs[0] * s_h * s_w)),\n",
    "                                                 (\"g_h0_lin_b\", (num_gfs[0] * s_h * s_w,))]))\n",
    "        self.gen_output_dims[\"g_h0_out\"] = (s_h, s_w)\n",
    "\n",
    "        self.disc_weight_dims = OrderedDict()\n",
    "        s_h, s_w = self.x_dim[0], self.x_dim[1]\n",
    "        num_dfs = [self.c_dim] + num_dfs\n",
    "        ks = disc_kernel_size\n",
    "        self.disc_kernel_sizes = [ks]\n",
    "        for layer in range(len(disc_strides)):\n",
    "            assert disc_strides[layer] <= 2, \"invalid stride\"\n",
    "            assert ks % 2 == 1, \"invalid kernel size\"\n",
    "            self.disc_weight_dims[\"d_h%i_W\" % layer] = (ks, ks, num_dfs[layer], num_dfs[layer+1])\n",
    "            self.disc_weight_dims[\"d_h%i_b\" % layer] = (num_dfs[layer+1],)\n",
    "            s_h, s_w = conv_out_size(s_h, disc_strides[layer]), conv_out_size(s_w, disc_strides[layer])\n",
    "            ks = kernel_sizer(ks, disc_strides[layer])\n",
    "            self.disc_kernel_sizes.append(ks)\n",
    "\n",
    "        self.disc_weight_dims.update(OrderedDict([(\"d_h_end_lin_W\", (num_dfs[-1] * s_h * s_w, num_dfs[-1])),\n",
    "                                                  (\"d_h_end_lin_b\", (num_dfs[-1],)),\n",
    "                                                  (\"d_h_out_lin_W\", (num_dfs[-1], self.K)),\n",
    "                                                  (\"d_h_out_lin_b\", (self.K,))]))\n",
    "\n",
    "\n",
    "        for k, v in list(self.gen_output_dims.items()):\n",
    "            print(\"%s: %s\" % (k, v))\n",
    "        print('****')\n",
    "        for k, v in list(self.gen_weight_dims.items()):\n",
    "            print(\"%s: %s\" % (k, v))\n",
    "        print('****')\n",
    "        for k, v in list(self.disc_weight_dims.items()):\n",
    "            print(\"%s: %s\" % (k, v))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def construct_nets(self):\n",
    "\n",
    "        self.num_disc_layers = 5\n",
    "        self.num_gen_layers = 5\n",
    "        self.d_batch_norm = AttributeDict([(\"d_bn%i\" % dbn_i, batch_norm(name='d_bn%i' % dbn_i)) for dbn_i in range(self.num_disc_layers)])\n",
    "        self.sup_d_batch_norm = AttributeDict([(\"sd_bn%i\" % dbn_i, batch_norm(name='sup_d_bn%i' % dbn_i)) for dbn_i in range(self.num_disc_layers)])\n",
    "        self.g_batch_norm = AttributeDict([(\"g_bn%i\" % gbn_i, batch_norm(name='g_bn%i' % gbn_i)) for gbn_i in range(self.num_gen_layers)])\n",
    "\n",
    "\n",
    "        s_h, s_w = self.x_dim[0], self.x_dim[1]\n",
    "        s_h2, s_w2 = conv_out_size(s_h, 2), conv_out_size(s_w, 2)\n",
    "        s_h4, s_w4 = conv_out_size(s_h2, 2), conv_out_size(s_w2, 2)\n",
    "        s_h8, s_w8 = conv_out_size(s_h4, 2), conv_out_size(s_w4, 2)\n",
    "        s_h16, s_w16 = conv_out_size(s_h8, 2), conv_out_size(s_w8, 2)\n",
    "\n",
    "        self.gen_output_dims = OrderedDict([(\"g_h0_out\", (s_h16, s_w16)),\n",
    "                                            (\"g_h1_out\", (s_h8, s_w8)),\n",
    "                                            (\"g_h2_out\", (s_h4, s_w4)),\n",
    "                                            (\"g_h3_out\", (s_h2, s_w2)),\n",
    "                                            (\"g_h4_out\", (s_h, s_w))])\n",
    "\n",
    "        \n",
    "        self.gen_weight_dims = OrderedDict([(\"g_h0_lin_W\", (self.z_dim, self.gf_dim * 8 * s_h16 * s_w16)),\n",
    "                                            (\"g_h0_lin_b\", (self.gf_dim * 8 * s_h16 * s_w16,)),\n",
    "                                            (\"g_h1_W\", (5, 5, self.gf_dim*4, self.gf_dim*8)),\n",
    "                                            (\"g_h1_b\", (self.gf_dim*4,)),\n",
    "                                            (\"g_h2_W\", (5, 5, self.gf_dim*2, self.gf_dim*4)),\n",
    "                                            (\"g_h2_b\", (self.gf_dim*2,)),\n",
    "                                            (\"g_h3_W\", (5, 5, self.gf_dim*1, self.gf_dim*2)),\n",
    "                                            (\"g_h3_b\", (self.gf_dim*1,)),\n",
    "                                            (\"g_h4_W\", (5, 5, self.c_dim, self.gf_dim*1)),\n",
    "                                            (\"g_h4_b\", (self.c_dim,))])\n",
    "\n",
    "        self.disc_weight_dims = OrderedDict([(\"d_h0_W\", (5, 5, self.c_dim, self.df_dim)),\n",
    "                                             (\"d_h0_b\", (self.df_dim,)),\n",
    "                                             (\"d_h1_W\", (5, 5, self.df_dim, self.df_dim*2)),\n",
    "                                             (\"d_h1_b\", (self.df_dim*2,)),\n",
    "                                             (\"d_h2_W\", (5, 5, self.df_dim*2, self.df_dim*4)),\n",
    "                                             (\"d_h2_b\", (self.df_dim*4,)),\n",
    "                                             (\"d_h3_W\", (5, 5, self.df_dim*4, self.df_dim*8)),\n",
    "                                             (\"d_h3_b\", (self.df_dim*8,)),\n",
    "                                             (\"d_h_end_lin_W\", (self.df_dim * 8 * s_h16 * s_w16, self.df_dim*4)),\n",
    "                                             (\"d_h_end_lin_b\", (self.df_dim*4,)),\n",
    "                                             (\"d_h_out_lin_W\", (self.df_dim*4, self.K)),\n",
    "                                             (\"d_h_out_lin_b\", (self.K,))])\n",
    "\n",
    "\n",
    "    def _get_optimizer(self, lr):\n",
    "        if self.optimizer == 'adam':\n",
    "            return tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5)\n",
    "        elif self.optimizer == 'sgd':\n",
    "            return tf.train.MomentumOptimizer(learning_rate=lr, momentum=0.5)\n",
    "        else:\n",
    "            raise ValueError(\"Optimizer must be either 'adam' or 'sgd'\")    \n",
    "\n",
    "    def initialize_wgts(self, scope_str):\n",
    "\n",
    "        if scope_str == \"generator\":\n",
    "            weight_dims = self.gen_weight_dims\n",
    "            numz = self.num_gen\n",
    "        elif scope_str == \"discriminator\":\n",
    "            weight_dims = self.disc_weight_dims\n",
    "            numz = self.num_disc\n",
    "        else:\n",
    "            raise RuntimeError(\"invalid scope!\")\n",
    "\n",
    "        param_list = []\n",
    "        with tf.variable_scope(scope_str) as scope:\n",
    "            for zi in range(numz):\n",
    "                for m in range(self.num_mcmc):\n",
    "                    wgts_ = AttributeDict()\n",
    "                    for name, shape in weight_dims.items():\n",
    "                        wgts_[name] = tf.get_variable(\"%s_%04d_%04d\" % (name, zi, m),\n",
    "                                                      shape, initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "                    param_list.append(wgts_)\n",
    "            return param_list\n",
    "        \n",
    "\n",
    "    def build_bgan_graph(self):\n",
    "    \n",
    "        self.inputs = tf.placeholder(tf.float32,\n",
    "                                     [self.batch_size] + list(self.x_dim), name='real_images')\n",
    "\n",
    "        self.z = tf.placeholder(tf.float32, [self.batch_size, self.z_dim, self.num_gen], name='z')\n",
    "        self.z_sampler = tf.placeholder(tf.float32, [self.batch_size, self.z_dim], name='z_sampler')\n",
    "        \n",
    "        # initialize generator weights\n",
    "        self.gen_param_list = self.initialize_wgts(\"generator\")\n",
    "        self.disc_param_list = self.initialize_wgts(\"discriminator\")\n",
    "        ### build discrimitive losses and optimizers\n",
    "        # prep optimizer args\n",
    "        self.d_learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "        \n",
    "        # compile all disciminative weights\n",
    "        t_vars = tf.trainable_variables()\n",
    "        self.d_vars = []\n",
    "        for di in range(self.num_disc):\n",
    "            for m in range(self.num_mcmc):\n",
    "                self.d_vars.append([var for var in t_vars if 'd_' in var.name and \"_%04d_%04d\" % (di, m) in var.name])\n",
    "\n",
    "        ### build disc losses and optimizers\n",
    "        self.d_losses, self.d_optims, self.d_optims_adam = [], [], []\n",
    "        for di, disc_params in enumerate(self.disc_param_list):\n",
    "\n",
    "            d_probs, d_logits, _ = self.discriminator(self.inputs, self.K, disc_params)\n",
    "\n",
    "            constant_labels = np.zeros((self.batch_size, 2))\n",
    "            constant_labels[:, 1] = 1.0  # real\n",
    "            d_loss_real = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=d_logits,\n",
    "                                                                                 labels=tf.constant(constant_labels)))\n",
    "\n",
    "            d_loss_fakes = []\n",
    "            for gi, gen_params in enumerate(self.gen_param_list):\n",
    "                d_probs_, d_logits_, _ = self.discriminator(self.generator(self.z[:, :, gi % self.num_gen], gen_params), \n",
    "                                                            self.K, disc_params)\n",
    "                constant_labels = np.zeros((self.batch_size, self.K))\n",
    "                constant_labels[:, 0] = 1.0 # class label indicating it came from generator, aka fake\n",
    "                d_loss_fake_ = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=d_logits_,\n",
    "                                                                                      labels=tf.constant(constant_labels)))\n",
    "                d_loss_fakes.append(d_loss_fake_)\n",
    "\n",
    "            d_losses = []\n",
    "            for d_loss_fake_ in d_loss_fakes:\n",
    "                d_loss_ = d_loss_real * float(self.num_gen) + d_loss_fake_\n",
    "                if not self.ml:\n",
    "                    d_loss_ += self.disc_prior(disc_params) + self.disc_noise(disc_params)\n",
    "                d_losses.append(tf.reshape(d_loss_, [1]))\n",
    "\n",
    "            # d_loss = tf.reduce_logsumexp(tf.concat(d_losses, 0))\n",
    "            d_loss = tf.reduce_logsumexp(tf.concat(d_losses, 0)) - tf.log(tf.cast(len(d_losses), tf.float32))\n",
    "            self.d_losses.append(d_loss)\n",
    "            d_opt = self._get_optimizer(self.d_learning_rate)\n",
    "            self.d_optims.append(d_opt.minimize(d_loss, var_list=self.d_vars[di]))\n",
    "            d_opt_adam = tf.train.AdamOptimizer(learning_rate=self.d_learning_rate, beta1=0.5)\n",
    "            self.d_optims_adam.append(d_opt_adam.minimize(d_loss, var_list=self.d_vars[di]))\n",
    "\n",
    "        ### build generative losses and optimizers\n",
    "        self.g_learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "        self.g_vars = []\n",
    "        for gi in range(self.num_gen):\n",
    "            for m in range(self.num_mcmc):\n",
    "                self.g_vars.append([var for var in t_vars if 'g_' in var.name and \"_%04d_%04d\" % (gi, m) in var.name])\n",
    "        \n",
    "        self.g_losses, self.g_optims, self.g_optims_adam = [], [], []\n",
    "        for gi, gen_params in enumerate(self.gen_param_list):\n",
    "\n",
    "            gi_losses = []\n",
    "            for disc_params in self.disc_param_list:\n",
    "                d_probs_, d_logits_, d_features_fake = self.discriminator(self.generator(self.z[:, :, gi % self.num_gen],\n",
    "                                                                                         gen_params),\n",
    "                                                                          self.K, disc_params)\n",
    "                _, _, d_features_real = self.discriminator(self.inputs, self.K, disc_params)\n",
    "                constant_labels = np.zeros((self.batch_size, self.K))\n",
    "                constant_labels[:, 1] = 1.0 # class label indicating that this fake is real\n",
    "                g_loss_ = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=d_logits_,\n",
    "                                                                                 labels=tf.constant(constant_labels)))\n",
    "                g_loss_ += tf.reduce_mean(huber_loss(d_features_real[-1], d_features_fake[-1]))\n",
    "                if not self.ml:\n",
    "                    g_loss_ += self.gen_prior(gen_params) + self.gen_noise(gen_params)\n",
    "                gi_losses.append(tf.reshape(g_loss_, [1]))\n",
    "                \n",
    "            # g_loss = tf.reduce_logsumexp(tf.concat(gi_losses, 0))\n",
    "            g_loss = tf.reduce_logsumexp(tf.concat(d_losses, 0)) - tf.log(tf.cast(len(d_losses), tf.float32))\n",
    "            self.g_losses.append(g_loss)\n",
    "            g_opt = self._get_optimizer(self.g_learning_rate)\n",
    "            self.g_optims.append(g_opt.minimize(g_loss, var_list=self.g_vars[gi]))\n",
    "            g_opt_adam = tf.train.AdamOptimizer(learning_rate=self.g_learning_rate, beta1=0.5)\n",
    "            self.g_optims_adam.append(g_opt_adam.minimize(g_loss, var_list=self.g_vars[gi]))\n",
    "\n",
    "        ### build samplers\n",
    "        self.gen_samplers = []\n",
    "        for gi, gen_params in enumerate(self.gen_param_list):\n",
    "            self.gen_samplers.append(self.generator(self.z_sampler, gen_params))\n",
    "\n",
    "\n",
    "    def discriminator(self, image, K, disc_params, train=True):\n",
    "\n",
    "        with tf.variable_scope(\"discriminator\") as scope:\n",
    "\n",
    "            h = image\n",
    "            for layer in range(len(self.disc_strides)):\n",
    "                if layer == 0:\n",
    "                    h = lrelu(conv2d(h,\n",
    "                                     self.disc_weight_dims[\"d_h%i_W\" % layer][-1],\n",
    "                                     name='d_h%i_conv' % layer,\n",
    "                                     k_h=self.disc_kernel_sizes[layer], k_w=self.disc_kernel_sizes[layer],\n",
    "                                     d_h=self.disc_strides[layer], d_w=self.disc_strides[layer],\n",
    "                                     w=disc_params[\"d_h%i_W\" % layer], biases=disc_params[\"d_h%i_b\" % layer]))\n",
    "                else:\n",
    "                    h = lrelu(self.d_batch_norm[\"d_bn%i\" % layer](conv2d(h,\n",
    "                                                                         self.disc_weight_dims[\"d_h%i_W\" % layer][-1],\n",
    "                                                                         name='d_h%i_conv' % layer,\n",
    "                                                                         k_h=self.disc_kernel_sizes[layer], k_w=self.disc_kernel_sizes[layer],\n",
    "                                                                         d_h=self.disc_strides[layer], d_w=self.disc_strides[layer],\n",
    "                                                                         w=disc_params[\"d_h%i_W\" % layer], biases=disc_params[\"d_h%i_b\" % layer]), train=train))\n",
    "\n",
    "            h_end = lrelu(linear(tf.reshape(h, [self.batch_size, -1]),\n",
    "                              self.df_dim*4, \"d_h_end_lin\",\n",
    "                              matrix=disc_params.d_h_end_lin_W, bias=disc_params.d_h_end_lin_b)) # for feature norm\n",
    "            h_out = linear(h_end, K,\n",
    "                           'd_h_out_lin',\n",
    "                           matrix=disc_params.d_h_out_lin_W, bias=disc_params.d_h_out_lin_b)\n",
    "            \n",
    "            return tf.nn.softmax(h_out), h_out, [h_end]\n",
    "            \n",
    "\n",
    "    def generator(self, z, gen_params):\n",
    "\n",
    "        with tf.variable_scope(\"generator\") as scope:\n",
    "\n",
    "            h = linear(z, self.gen_weight_dims[\"g_h0_lin_W\"][-1], 'g_h0_lin',\n",
    "                       matrix=gen_params.g_h0_lin_W, bias=gen_params.g_h0_lin_b)\n",
    "            h = tf.nn.relu(self.g_batch_norm.g_bn0(h))\n",
    "\n",
    "            h = tf.reshape(h, [self.batch_size, self.gen_output_dims[\"g_h0_out\"][0],\n",
    "                               self.gen_output_dims[\"g_h0_out\"][1], -1])\n",
    "\n",
    "            for layer in range(1, len(self.gen_strides)+1):\n",
    "\n",
    "                out_shape = [self.batch_size, self.gen_output_dims[\"g_h%i_out\" % layer][0],\n",
    "                             self.gen_output_dims[\"g_h%i_out\" % layer][1], self.gen_weight_dims[\"g_h%i_W\" % layer][-2]]\n",
    "\n",
    "                h = deconv2d(h,\n",
    "                             out_shape,\n",
    "                             k_h=self.gen_kernel_sizes[layer-1], k_w=self.gen_kernel_sizes[layer-1],\n",
    "                             d_h=self.gen_strides[layer-1], d_w=self.gen_strides[layer-1],\n",
    "                             name='g_h%i' % layer,\n",
    "                             w=gen_params[\"g_h%i_W\" % layer], biases=gen_params[\"g_h%i_b\" % layer])\n",
    "                if layer < len(self.gen_strides):\n",
    "                    h = tf.nn.relu(self.g_batch_norm[\"g_bn%i\" % layer](h))\n",
    "\n",
    "            return tf.nn.tanh(h)        \n",
    "\n",
    "\n",
    "    def gen_prior(self, gen_params):\n",
    "        with tf.variable_scope(\"generator\") as scope:\n",
    "            prior_loss = 0.0\n",
    "            for var in list(gen_params.values()):\n",
    "                nn = tf.divide(var, self.prior_std)\n",
    "                prior_loss += tf.reduce_mean(tf.multiply(nn, nn))\n",
    "                \n",
    "        prior_loss /= self.batch_size\n",
    "\n",
    "        return prior_loss\n",
    "\n",
    "    def gen_noise(self, gen_params): \n",
    "        with tf.variable_scope(\"generator\") as scope:\n",
    "            noise_loss = 0.0\n",
    "            for name, var in gen_params.items():\n",
    "                # noise_ = tfp.distributions.Normal(loc=0., scale=self.noise_std*tf.ones(var.get_shape()))\n",
    "                # noise_loss += tf.reduce_sum(var * noise_.sample())\n",
    "                 noise_loss += tf.reduce_sum(var * tf.random.normal(tf.shape(var), mean=0., stddev=self.noise_std))\n",
    "        noise_loss /= self.dataset_size\n",
    "        return noise_loss\n",
    "\n",
    "    def disc_prior(self, disc_params):\n",
    "        with tf.variable_scope(\"discriminator\") as scope:\n",
    "            prior_loss = 0.0\n",
    "            for var in list(disc_params.values()):\n",
    "                nn = tf.divide(var, self.prior_std)\n",
    "                prior_loss += tf.reduce_mean(tf.multiply(nn, nn))\n",
    "                \n",
    "        prior_loss /= self.batch_size\n",
    "\n",
    "        return prior_loss\n",
    "\n",
    "    def disc_noise(self, disc_params): \n",
    "        with tf.variable_scope(\"discriminator\") as scope:\n",
    "            noise_loss = 0.0\n",
    "            for var in list(disc_params.values()):\n",
    "                # noise_ = tfp.distributions.Normal(loc=0., scale=self.noise_std*tf.ones(var.get_shape()))\n",
    "                # noise_loss += tf.reduce_sum(var * noise_.sample())\n",
    "                 noise_loss += tf.reduce_sum(var * tf.random.normal(tf.shape(var), mean=0., stddev=self.noise_std))\n",
    "        noise_loss /= self.dataset_size\n",
    "        return noise_loss        \n",
    "\n",
    "\n",
    "\n",
    "def one_hot_encoded(class_numbers, num_classes):\n",
    "    return np.eye(num_classes, dtype=float)[class_numbers]\n",
    "\n",
    "class AttributeDict(dict):\n",
    "    def __getattr__(self, attr):\n",
    "        return self[attr]\n",
    "    def __setattr__(self, attr, value):\n",
    "        self[attr] = value\n",
    "    def __hash__(self):\n",
    "        return hash(tuple(sorted(self.items())))\n",
    "        \n",
    "class SynthDataset():\n",
    "    \n",
    "    def __init__(self, x_dim=100, num_clusters=10, seed=1234):\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.x_dim = x_dim\n",
    "        self.N = 10000\n",
    "        self.true_z_dim = 2\n",
    "        # generate synthetic data\n",
    "        self.Xs = []\n",
    "        for _ in range(num_clusters):\n",
    "            cluster_mean = np.random.randn(self.true_z_dim) * 5 # to make them more spread\n",
    "            A = np.random.randn(self.x_dim, self.true_z_dim) * 5\n",
    "            X = np.dot(np.random.randn(self.N // num_clusters, self.true_z_dim) + cluster_mean,A.T)\n",
    "            self.Xs.append(X)\n",
    "        X_raw = np.concatenate(self.Xs)\n",
    "        self.X = (X_raw - X_raw.mean(0)) / (X_raw.std(0))\n",
    "        print(self.X.shape)\n",
    "        \n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "\n",
    "        rand_idx = np.random.choice(list(range(self.N)), size=(batch_size,), replace=False)\n",
    "        return self.X[rand_idx]\n",
    "\n",
    "\n",
    "\n",
    "class batch_norm(object):\n",
    "    def __init__(self, name=\"batch_norm\"):\n",
    "        self.bn = BatchNormalization(momentum=0.9, epsilon=1e-5, name=name)\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        return self.bn(x, training=train)\n",
    "\n",
    "\n",
    "def linear(input_, output_size, name=None, stddev=0.02, bias_start=0.0, matrix=None, bias=None):\n",
    "    with tf.variable_scope(name or \"Linear\"):\n",
    "        if matrix is None:\n",
    "            matrix = tf.get_variable(\"Matrix\", [input_.get_shape()[1], output_size],\n",
    "                                     dtype=tf.float32,\n",
    "                                     initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "        if bias is None:\n",
    "            bias = tf.get_variable(\"bias\", [output_size],\n",
    "                                   initializer=tf.constant_initializer(bias_start))\n",
    "        return tf.matmul(input_, matrix) + bias\n",
    "\n",
    "\n",
    "\n",
    "def b_dcgan(dataset, args):\n",
    "\n",
    "    z_dim = args.z_dim\n",
    "    x_dim = dataset.x_dim\n",
    "    batch_size = args.batch_size\n",
    "    dataset_size = dataset.dataset_size\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    # if tf.get_default_session() is not None:\n",
    "    # tf.get_default_session().close()\n",
    "\n",
    "    session = tf.InteractiveSession()\n",
    "\n",
    "    all_d_losses = []\n",
    "    all_g_losses = []\n",
    "\n",
    "    tf.set_random_seed(args.random_seed)\n",
    "    \n",
    "    dcgan = BDCGAN(x_dim, z_dim, dataset_size, batch_size=batch_size,\n",
    "                   J=args.J, J_d=args.J_d, M=args.M,\n",
    "                   num_layers=args.num_layers,\n",
    "                   lr=args.lr, optimizer=args.optimizer, gf_dim=args.gf_dim, \n",
    "                   df_dim=args.df_dim,\n",
    "                   ml=(args.ml and args.J==1 and args.M==1 and args.J_d==1))\n",
    "    \n",
    "    print(\"Starting session\")\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    print(\"Starting training loop\")\n",
    "        \n",
    "    num_train_iter = args.train_iter\n",
    "\n",
    "    optimizer_dict = {\"disc\": dcgan.d_optims_adam,\n",
    "                      \"gen\": dcgan.g_optims_adam}\n",
    "\n",
    "    base_learning_rate = args.lr # for now we use same learning rate for Ds and Gs\n",
    "    lr_decay_rate = args.lr_decay\n",
    "    num_disc = args.J_d\n",
    "    \n",
    "    for train_iter in range(num_train_iter):\n",
    "\n",
    "        if train_iter == 5000:\n",
    "            print(\"Switching to user-specified optimizer\")\n",
    "            optimizer_dict = {\"disc\": dcgan.d_optims_adam,\n",
    "                              \"gen\": dcgan.g_optims_adam}\n",
    "\n",
    "        learning_rate = base_learning_rate * np.exp(-lr_decay_rate *\n",
    "                                                    min(1.0, (train_iter*batch_size)/float(dataset_size)))\n",
    "\n",
    "        image_batch = dataset.next_batch()\n",
    "     \n",
    "\n",
    "        ### compute disc losses\n",
    "        batch_z = np.random.uniform(-1, 1, [batch_size, z_dim, dcgan.num_gen])\n",
    "        disc_info = session.run(optimizer_dict[\"disc\"] + dcgan.d_losses, \n",
    "                                feed_dict={dcgan.inputs: image_batch,\n",
    "                                           dcgan.z: batch_z,\n",
    "                                           dcgan.d_learning_rate: learning_rate})\n",
    "\n",
    "        d_losses = disc_info[num_disc:num_disc*2]\n",
    "\n",
    "        ### compute generative losses\n",
    "        batch_z = np.random.uniform(-1, 1, [batch_size, z_dim, dcgan.num_gen])\n",
    "        gen_info = session.run(optimizer_dict[\"gen\"] + dcgan.g_losses,\n",
    "                               feed_dict={dcgan.z: batch_z,\n",
    "                                          dcgan.inputs: image_batch,\n",
    "                                          dcgan.g_learning_rate: learning_rate})\n",
    "        g_losses = [g_ for g_ in gen_info if g_ is not None]\n",
    "\n",
    "        if train_iter > 0 and train_iter % args.n_save == 0:\n",
    "\n",
    "            def safe_print_losses(label, losses):\n",
    "                if isinstance(losses, (list, tuple)):\n",
    "                    cleaned = [float(x) for x in losses if x is not None]\n",
    "                    if cleaned:\n",
    "                        print(\"{} = {}\".format(label, \", \".join([\"%.2f\" % x for x in cleaned])))\n",
    "                    else:\n",
    "                        print(\"{}: None\".format(label))\n",
    "                else:\n",
    "                    if losses is not None:\n",
    "                        print(\"{} = {:.2f}\".format(label, float(losses)))\n",
    "                    else:\n",
    "                        print(\"{}: None\".format(label))\n",
    "\n",
    "            safe_print_losses(\"Disc losses\", d_losses)\n",
    "            safe_print_losses(\"Gen losses\", g_losses)\n",
    "            \n",
    "                    # Collect losses for tracking\n",
    "            if d_losses and g_losses:\n",
    "                all_d_losses.append(np.mean(d_losses))\n",
    "                all_g_losses.append(np.mean(g_losses))\n",
    "\n",
    "            print(\"saving results and samples\")\n",
    "\n",
    "            # Save results safely\n",
    "            results = {\n",
    "                \"disc_losses\": [float(x) for x in d_losses if x is not None] if d_losses else [],\n",
    "                \"gen_losses\": [float(x) for x in g_losses if x is not None] if g_losses else []\n",
    "            }\n",
    "\n",
    "            import pandas as pd\n",
    "\n",
    "            # Save results to CSV\n",
    "            # Safely handle missing discriminator losses\n",
    "            gen_losses = results.get(\"gen_losses\", [])\n",
    "            disc_losses = results.get(\"disc_losses\", [])\n",
    "\n",
    "            # If discriminator losses are missing, fill with NaNs\n",
    "            if len(disc_losses) == 0:\n",
    "                disc_losses = [np.nan] * len(gen_losses)\n",
    "\n",
    "            # Make sure they are same length\n",
    "            min_len = min(len(disc_losses), len(gen_losses))\n",
    "            disc_losses = disc_losses[:min_len]\n",
    "            gen_losses = gen_losses[:min_len]\n",
    "\n",
    "            # Save as DataFrame\n",
    "            df = pd.DataFrame({\n",
    "                \"disc_loss\": disc_losses,\n",
    "                \"gen_loss\": gen_losses\n",
    "            })\n",
    "            df.to_csv(os.path.join(args.out_dir, \"training_losses.csv\"), index=False)\n",
    "            print(\"Saved training losses to CSV.\")\n",
    "\n",
    "\n",
    "            with open(os.path.join(args.out_dir, 'results_%i.json' % train_iter), 'w') as fp:\n",
    "                json.dump(results, fp)\n",
    "\n",
    "            if args.save_weights:\n",
    "                var_dict = {}\n",
    "                for var in tf.trainable_variables():\n",
    "                    var_dict[var.name] = session.run(var) \n",
    "                np.savez_compressed(os.path.join(args.out_dir, \"weights_%i.npz\" % train_iter), **var_dict)\n",
    "            \n",
    "            print(f\"Step {train_iter}: D = {np.mean(d_losses):.4f}, G = {np.mean(g_losses):.4f}\")\n",
    "            saver.save(session, os.path.join(args.out_dir, \"model.ckpt\"))\n",
    "            print(\"Training complete. Model saved to: {}\".format(args.out_dir))\n",
    "            \n",
    "            # Save full training losses after loop\n",
    "            training_df = pd.DataFrame({\n",
    "                \"disc_loss\": all_d_losses,\n",
    "                \"gen_loss\": all_g_losses\n",
    "                })\n",
    "            training_df.to_csv(os.path.join(args.out_dir, \"training_losses_full.csv\"), index=False)\n",
    "            print(\"Saved full training loss history.\")\n",
    "            \n",
    "# After training loop\n",
    "# saver = tf.train.Saver()\n",
    "print(\"done\")\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    out_dir=\"Results\",\n",
    "    n_save=5000,\n",
    "    z_dim=100,\n",
    "    gf_dim=64,\n",
    "    df_dim=96,\n",
    "    data_path=\"raw (FX + EQ).csv\",\n",
    "    dataset=\"FXEQDataset\",\n",
    "    batch_size=64,\n",
    "    prior_std=1.0,\n",
    "    num_layers=4,\n",
    "    J=1,\n",
    "    J_d=1,\n",
    "    M=1,\n",
    "    N=128,\n",
    "    train_iter=50000,\n",
    "    wasserstein=False,\n",
    "    ml=False,\n",
    "    save_samples=False,\n",
    "    save_weights=False,\n",
    "    random_seed=2222,\n",
    "    lr=0.005,\n",
    "    lr_decay=3.0,\n",
    "    optimizer=\"sgd\"\n",
    ")\n",
    "\n",
    "    \n",
    "# args = parser.parse_args()\n",
    "np.random.seed(args.random_seed)\n",
    "tf.set_random_seed(args.random_seed)\n",
    "\n",
    "if not os.path.exists(args.out_dir):\n",
    "    os.makedirs(args.out_dir)\n",
    "args.out_dir = os.path.join(args.out_dir, f\"bgan_{args.dataset}_{int(time.time())}\")\n",
    "os.makedirs(args.out_dir)\n",
    "\n",
    "# Save config\n",
    "import pprint\n",
    "with open(os.path.join(args.out_dir, \"hypers.txt\"), \"w\") as hf:\n",
    "    hf.write(\"Hyper settings:\\n\")\n",
    "    hf.write(\"%s\\n\" % (pprint.pformat(args.__dict__)))\n",
    "\n",
    "\n",
    "    # set seeds\n",
    "np.random.seed(args.random_seed)\n",
    "tf.set_random_seed(args.random_seed)\n",
    "\n",
    "# if not os.path.exists(args.out_dir):\n",
    "#         print(\"Creating %s\" % args.out_dir)\n",
    "#         os.makedirs(args.out_dir)\n",
    "# args.out_dir = os.path.join(args.out_dir, \"bgan_%s_%i\" % (args.dataset, int(time.time())))\n",
    "# os.makedirs(args.out_dir)\n",
    "\n",
    "import pprint\n",
    "with open(os.path.join(args.out_dir, \"hypers.txt\"), \"w\") as hf:\n",
    "        hf.write(\"Hyper settings:\\n\")\n",
    "        hf.write(\"%s\\n\" % (pprint.pformat(args.__dict__)))\n",
    "\n",
    "if args.dataset.lower() == \"fxeqdataset\":\n",
    "        df = pd.read_csv(args.data_path)\n",
    "        dataset = FXEQDataset(data=df.values, batch_size=args.batch_size)\n",
    "else:\n",
    "        raise RuntimeError(\"invalid dataset %s\" % args.dataset)\n",
    "        \n",
    "\n",
    "\n",
    "# df = pd.read_csv(args.data_path)\n",
    "# dataset = FXEQDataset(data=df.values, batch_size=args.batch_size) # Train on raw prices\n",
    "# dataset = FXEQDataset(data=scaled_data, batch_size=args.batch_size) # Train on scaled prices\n",
    " \n",
    "# Train on scaled returns\n",
    "args.use_returns = True\n",
    "args.use_scaled = True\n",
    "\n",
    "if args.use_returns:\n",
    "    data = np.diff(np.log(df.values + 1e-5), axis=0)\n",
    "else:\n",
    "    data = df.values\n",
    "\n",
    "if args.use_scaled:\n",
    "    data = StandardScaler().fit_transform(data)\n",
    "\n",
    "dataset = FXEQDataset(data=data, batch_size=args.batch_size)\n",
    "\n",
    "\n",
    "b_dcgan(dataset, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65c1760-9cd3-4f4e-938e-62d0956e660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "# CPU and RAM usage\n",
    "print(f\"CPU Usage: {psutil.cpu_percent()}%\")\n",
    "print(f\"RAM Usage: {psutil.virtual_memory().percent}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66094ae-8aa8-471f-a75e-64c0c95ea065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Path to your results CSV\n",
    "result_dir = \"Results/bgan_FXEQDataset_1748028009/bgan_FXEQDataset_1748028009\"\n",
    "loss_path = os.path.join(result_dir, \"training_losses.csv\")\n",
    "\n",
    "# Load and check the CSV\n",
    "loss_df = pd.read_csv(loss_path)\n",
    "print(\"CSV Preview:\")\n",
    "print(loss_df.head())   # Show first few rows\n",
    "print(\"Data types:\")\n",
    "print(loss_df.dtypes)   # Confirm it's float\n",
    "\n",
    "# Check for NaNs\n",
    "print(\"NaNs in disc_loss:\", loss_df['disc_loss'].isna().sum())\n",
    "print(\"NaNs in gen_loss:\", loss_df['gen_loss'].isna().sum())\n",
    "\n",
    "# Fix index to match iterations (e.g., every 100 steps)\n",
    "steps = list(range(100, 100 * (len(loss_df) + 1), 100))\n",
    "\n",
    "# Plot the actual curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(steps, loss_df['disc_loss'], label='Discriminator Loss')\n",
    "plt.plot(steps, loss_df['gen_loss'], label='Generator Loss')\n",
    "plt.title(\"Training Loss Curves\")\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184b19d6-91e3-4146-a198-e16aeaab960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load trained model\n",
    "ckpt_path = os.path.join(result_dir, \"model.ckpt\")\n",
    "\n",
    "# Load and normalize original data (for inverse scaling later)\n",
    "df_real = pd.read_csv(\"raw (FX + EQ).csv\").drop(columns=[\"Date\"], errors=\"ignore\").dropna()\n",
    "real_values = df_real.values\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(real_values)\n",
    "\n",
    "# Generate samples\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Rebuild model exactly as trained\n",
    "dataset_size = real_values.shape[0]\n",
    "x_dim = [4, 4, 1]  # from g_h4_out size\n",
    "z_dim = 100\n",
    "batch_size = 64\n",
    "\n",
    "model = BDCGAN(x_dim=x_dim, z_dim=z_dim, dataset_size=dataset_size, batch_size=batch_size)\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, ckpt_path)\n",
    "\n",
    "# Generate synthetic samples\n",
    "z_input = np.random.uniform(-1, 1, [batch_size, z_dim])\n",
    "synthetic_imgs = sess.run(model.gen_samplers[0], feed_dict={model.z_sampler: z_input})\n",
    "\n",
    "# Flatten and inverse scale\n",
    "synthetic_flat = synthetic_imgs.reshape(batch_size, -1)\n",
    "synthetic_scaled_back = scaler.inverse_transform(synthetic_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc85de96-1a5f-4293-b5fb-d64fe906223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Select a few dimensions (features) to compare\n",
    "features_to_plot = [0, 1, 2]  # Adjust to match your columns\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, col in enumerate(features_to_plot):\n",
    "    plt.subplot(1, len(features_to_plot), i+1)\n",
    "    sns.kdeplot(real_values[:, col], label='Real', fill=True)\n",
    "    sns.kdeplot(synthetic_scaled_back[:, col], label='Synthetic', fill=True)\n",
    "    plt.title(f'Distribution - Feature {col}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09874dff-f2c3-42ea-a84f-ac9c5eeb71e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import time\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from tensorflow.compat.v1.keras.layers import Dense\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()-\n",
    "\n",
    "# # ----------------------- Data Preprocessing -----------------------\n",
    "\n",
    "# df = pd.read_csv(\"raw (FX + EQ).csv\")\n",
    "# df = df.drop(columns=[\"Date\"], errors=\"ignore\")  # Drop non-numeric columns\n",
    "# df = df.dropna(axis=0)  # Drop rows with NaNs\n",
    "# data = df.values\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# # ----------------------- Dataset Class -----------------------\n",
    "\n",
    "# class FXEQDataset:\n",
    "#     def __init__(self, data, batch_size):\n",
    "#         self.data = data\n",
    "#         self.batch_size = batch_size\n",
    "#         self.idx = 0\n",
    "#         self.num_samples = data.shape[0]\n",
    "\n",
    "#         # âœ… Add these two lines:\n",
    "#         self.x_dim = [data.shape[1]]  # or data.shape[1:] if using multi-dimensional input\n",
    "#         self.dataset_size = self.num_samples\n",
    "\n",
    "#     def next_batch(self):\n",
    "#         if self.idx + self.batch_size > self.num_samples:\n",
    "#             self.idx = 0\n",
    "#         batch = self.data[self.idx:self.idx + self.batch_size]\n",
    "#         self.idx += self.batch_size\n",
    "#         return batch\n",
    "\n",
    "\n",
    "# # ----------------------- Minimal BayesGAN Mockup -----------------------\n",
    "\n",
    "# class MiniGAN:\n",
    "#     def __init__(self, x_dim, z_dim, batch_size, lr):\n",
    "#         self.x_dim = x_dim\n",
    "#         self.z_dim = z_dim\n",
    "#         self.batch_size = batch_size\n",
    "#         self.lr = lr\n",
    "#         self._build_model()\n",
    "\n",
    "#     def _build_generator(self, z):\n",
    "#         with tf.variable_scope(\"generator\", reuse=tf.AUTO_REUSE):\n",
    "#             h1 = Dense(128, activation=tf.nn.relu)(z)\n",
    "#             out = tf.layers.dense(h1, self.x_dim)\n",
    "#         return out\n",
    "\n",
    "#     def _build_discriminator(self, x):\n",
    "#         with tf.variable_scope(\"discriminator\", reuse=tf.AUTO_REUSE):\n",
    "#             h1 = Dense(128, activation=tf.nn.relu)(z)\n",
    "#             logits = tf.layers.dense(h1, 1)\n",
    "#         return logits\n",
    "\n",
    "#     def _build_model(self):\n",
    "#         self.z = tf.placeholder(tf.float32, [None, self.z_dim], name=\"z\")\n",
    "#         self.x = tf.placeholder(tf.float32, [None, self.x_dim], name=\"x\")\n",
    "\n",
    "#         self.fake_x = self._build_generator(self.z)\n",
    "#         real_logits = self._build_discriminator(self.x)\n",
    "#         fake_logits = self._build_discriminator(self.fake_x)\n",
    "\n",
    "#         # Losses\n",
    "#         self.d_loss = tf.reduce_mean(\n",
    "#             tf.nn.sigmoid_cross_entropy_with_logits(logits=real_logits, labels=tf.ones_like(real_logits))) + \\\n",
    "#                       tf.reduce_mean(\n",
    "#                           tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=tf.zeros_like(fake_logits)))\n",
    "\n",
    "#         self.g_loss = tf.reduce_mean(\n",
    "#             tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=tf.ones_like(fake_logits)))\n",
    "\n",
    "#         # Variables\n",
    "#         all_vars = tf.trainable_variables()\n",
    "#         self.d_vars = [v for v in all_vars if \"discriminator\" in v.name]\n",
    "#         self.g_vars = [v for v in all_vars if \"generator\" in v.name]\n",
    "\n",
    "#         # Optimizers\n",
    "#         self.d_opt = tf.train.AdamOptimizer(self.lr).minimize(self.d_loss, var_list=self.d_vars)\n",
    "#         self.g_opt = tf.train.AdamOptimizer(self.lr).minimize(self.g_loss, var_list=self.g_vars)\n",
    "\n",
    "# # ----------------------- Training Function -----------------------\n",
    "\n",
    "# def train_gan(data, args):\n",
    "#     tf.reset_default_graph()\n",
    "#     dataset = FXEQDataset(data, batch_size=args[\"batch_size\"])\n",
    "#     if tf.get_default_session() is not None:\n",
    "#     tf.get_default_session().close()\n",
    "\n",
    "#     session = tf.InteractiveSession()\n",
    "\n",
    "#     tf.set_random_seed(args[\"random_seed\"])\n",
    "\n",
    "#     model = MiniGAN(x_dim=data.shape[1],\n",
    "#                     z_dim=args[\"z_dim\"],\n",
    "#                     batch_size=args[\"batch_size\"],\n",
    "#                     lr=args[\"lr\"])\n",
    "\n",
    "#     session.run(tf.global_variables_initializer())\n",
    "#     print(\"Training started...\")\n",
    "\n",
    "#     for step in range(args[\"train_iter\"]):\n",
    "#         batch_x, _ = dataset.next_batch()\n",
    "#         batch_z = np.random.uniform(-1, 1, [args[\"batch_size\"], args[\"z_dim\"]])\n",
    "\n",
    "#         # Train discriminator\n",
    "#         _ = session.run(model.d_opt, feed_dict={model.x: batch_x, model.z: batch_z})\n",
    "\n",
    "#         # Train generator\n",
    "#         _ = session.run(model.g_opt, feed_dict={model.z: batch_z})\n",
    "\n",
    "#         if step % args[\"n_save\"] == 0:\n",
    "#             d_loss_val, g_loss_val = session.run([model.d_loss, model.g_loss],\n",
    "#                                                  feed_dict={model.x: batch_x, model.z: batch_z})\n",
    "#             print(f\"Step {step}: D_loss = {d_loss_val:.4f}, G_loss = {g_loss_val:.4f}\")\n",
    "\n",
    "#     print(\"Training complete.\")\n",
    "#     session.close()\n",
    "\n",
    "# # ----------------------- Run -----------------------\n",
    "\n",
    "# args = {\n",
    "#     \"batch_size\": 64,\n",
    "#     \"z_dim\": 100,\n",
    "#     \"lr\": 0.001,\n",
    "#     \"train_iter\": 1000,\n",
    "#     \"n_save\": 100,\n",
    "#     \"random_seed\": 42,\n",
    "# }\n",
    "\n",
    "# train_gan(scaled_data, args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89774a5b-972a-4626-999d-4e3aa3547969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
